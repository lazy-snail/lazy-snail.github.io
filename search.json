[{"title":"database","url":"/cs/db/","content":"索引索引是什么？多数博客会用“书的目录”作解释，这是个很好的例子，为小白科普CS常识的时候可以用。作为开发可以从另一个角度理解：假设我们有100w行单列的数据，比如人名，如何提高“找到特定名字”的效率呢？简单直接地，我们想到的是树结构来存储，比如最熟悉的BST，它的查找效率非常高，lg(n)级别的查询次数，百万数据的也只需要20次左右，但是考虑到数据是持久化到存储介质的，和内存、CPU存在巨大的执行速度的差异，20次依然太多了。继续优化的方向并没有转变，专门为数据库而设计的各种B-tree应运而生，简单理解就是，从原来的二分，所需数据在左侧或者右侧，改为比如，按首字母序，分为26块，所需数据肯定在26中的某一块，这样一次查找就可以把范围缩小到数据量的1&#x2F;26。其他诸如平衡性、B树到B+树、聚簇&#x2F;非聚簇、聚集&#x2F;非聚集（注意和前一组不同）等概念，也都是在此基础上，结合软硬件特性所提出来的“优化手段”。（这里未讨论hash索引、位图索引、全文索引等特殊场景下的特殊方案。）\n为什么要有索引上面这段话好像没有说明白为啥要有索引，我直接对100w数据排序到硬盘不就行了？直接排序在单列数据当然是可行的，且操作效率也都很高。问题是数据库（问这个问题的应该回去重学数据库）表有很多字段，如果表只有一个索引，自然可以这样做。如果想添加两个&#x2F;多个索引，又不能同时按照两种方式对数据进行排序，比如一个关于姓名的索引和一个电话号码索引，这种情况就无法排序。将索引从数据行中整体分离出来，就可以创建多个索引，并且不需要对原始数据排序，但搜索过程即先在索引中找到对应值，然后根据匹配的记录找到对应的数据行。此外，索引行数据通常比表里的数据行更短，插入&#x2F;删除值时，为保持排序顺序，来回移动较短的索引值，比来回移动较长的数据行更加容易。\nIO 设备是计算机系统的瓶颈索引是数据库中用来优化查询速度的最为重要的技术手段。\n常用索引类型B+索引首先从大家默认的MySQL数据库默认的InnoDB存储引擎默认的用户创建索引类型说起：B+索引。它是目前关系型数据库系统中查找最为常用和最为有效的索引（大概是InnoDB团队自己说的）。B+索引的发展和演变过程就是上面提到的那样，就是B-tree的一类变种。\n这就不可避免地要继续延伸索引的发展史了，从BST开始，因缺乏社交活动而时间精力异常充沛的程序员（中的大神）们尝试了很多索引方案，BST 很快显露出其不仅仅在作为索引上的短板：最差性能是线性的；于是新的轮子–AVL 诞生了，它解决了平衡性的问题，把一棵树塞得满满的，新的问题又来了：维护平衡的代价太高，继续造轮子——RBT（红黑树），这棵树已经非常优秀了，被广泛应用于STL、linux进程调度、IO多路复用（epoll）、nginx、java TreeMap等；看上去拿来作索引的底层数据结构也挺好，很快新的问题又来了：还是反复提到的，硬盘这个猪队友的速度实在太拖后腿，以至于这么优秀的轮子依然不能用，百万量级就需要20多次磁盘访问，这之间巨大的数据鸿沟是第一段内容提到的，具体就是至少为纳秒与毫秒的差距。具体可以看下这里：让 CPU 告诉你硬盘和网络到底有多慢\nB-treeB-tree就是为了磁盘或其它存储设备而设计的一种平衡多路查找树(相对于二叉，每个内节点有多个分支)。B-tree已经具有了“有效降低磁盘查询次数”的特点，具有 高扇出性 的特点，一般高度在 2-4 层，意即查找某一键值的行记录最多只需要 2-4 次 I&#x2F;O。并且具有以下特点：\n\n定义任意非叶子结点最多只有M个儿子，且M&gt;2；\n根结点的儿子数为[2, M]；\n除根结点以外的非叶子结点的儿子数为[M&#x2F;2, M]；\n每个结点存放至少M&#x2F;2-1（取上整）和至多M-1个关键字；（至少2个关键字）\n非叶子结点的关键字个数&#x3D;指向儿子的指针个数-1；\n非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]；\n非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树；\n所有叶子结点位于同一层；\n\n它最终慢慢退出，或者说新的更合适的B+索引取而代之的原因是：B-tree在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。在数据库中基于范围的查询是非常频繁的，而B-tree对于此类操作效率太低，因为它在节点中存储索引和数据，而数据域的存在导致B-tree一次能够读入内存的数据范围减少（相比于只把索引读入内存），从而在”减少磁盘IO“这一关键优化上依然存在很大空间。B+索引也就是B-tree的基础上，解决了这一问题：把数据域转移到叶子节点，所有非叶子节点只保存索引。\nB+tree在B-tree的基础上进行改造，具有不同于后者的特性：\n\n非叶子节点的子树指针与关键字个数相同；\n非叶子节点的子树指针p[i],指向关键字值属于[k[i],k[i+1]]的子树.(B树是开区间,也就是说B树不允许关键字重复,B+树允许重复)；\n为所有叶子节点增加一个链指针；\n所有关键字都在叶子节点出现(稠密索引). (且链表中的关键字恰好是有序的)；\n非叶子节点相当于是叶子节点的索引(稀疏索引),叶子节点相当于是存储(关键字)数据的数据层；\n更适合于文件系统；\n\nB+tree的优点，也就是它更适合作索引数据结构的原因：\n\nB+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。\nB+树的查询效率更加稳定：由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。\n由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。\n\nB+ 树索引并不能找到一个给定键值的具体行，而是查找该行所在的页，然后通过把页读入内存，再在内存中进行查找，最后得到要查找的数据行。且可以进一步分为聚集索引（Clustered Index）和辅助索引（Secondary Index），叶子节点存放着所有的数据，不同之处是，叶子节点存放的是否是一整行的信息。\n聚集索引按照每张表的主键构造一棵 B+ 树，同时叶子节点中存放的即为整张表的行记录数据，也将聚集索引的叶子节点成为数据页。聚集索引的这个特性决定了索引组织表中数据也是索引的一部分。同 B+ 树数据结构一样，每个数据页都通过一个双向链表来进行链接。由于实际的数据页只能按照一棵 B+ 树进行排序，因此每张表只能拥有一个聚集索引。查询优化器倾向于采用聚集索引，因为能够在叶子节点上直接找到数据，对于主键的排序查找和范围查找速度都非常快。聚集索引的存储并不是物理上连续的（维护成本将非常高），而是逻辑上连续的。\n辅助索引叶子节点并不包含行记录的全部数据，除了包含键值以外，每个叶子节点的索引行中还包含了一个书签，用来告诉InnoDB存储引擎哪里可以找到与索引相对应的行数据。辅助索引的存在并不影响数据在聚集索引中的组织，因此每张表上可以有多个辅助索引。当通过辅助索引来查找数据时，InnoDB存储引擎会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后通过主键索引来找到一个完整的行记录。例：如果在一棵高度为3的辅助索引树中查找数据，那么需要对这棵辅助索引树遍历3次找到指定主键，如果聚集索引树的高度同样为3，那么还需要对聚集索引树进行3次查找，最终找到一个完整的行数据所在的页，因此共需要6次逻辑IO访问以得到最终的一个数据页。\nHash索引基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（Hash Code），哈希码是一个比较小的值，并且不同键值的行计算出来的哈希码是不同的。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。MySQL中，只有 Memory 引擎显式支持哈希索引，也是 Memory 引擎表默认索引类型，并且支持非唯一哈希索引。如果多个列的哈希值相同，索引会以链表的方式存放多个记录指针到同一个哈希条目中，所以哈希索引适合于精确查找；InnoDB 引擎支持的哈希索引是自适应的，会根据表的使用情况自动为表生成哈希索引，不能人为干预是否在一张表中生成哈希索引。\n位图索引（BitMap BitMap index）建立B+树索引的条件：高选择性的列(后续内容）。对于低选择性的列（这里应该描述成：只有固定几个值可选的列）如性别、婚否等，位图索引可能是个好的选择：适用场景\n\n只有几个固定值的列；\n不会频繁更新；位图索引原理\n\n全文索引（Full-Text Search Index)将存储于数据库中的整本书或整篇文章中的任意内容信息查找出来的技术。MyISAM支持全文索引，InnoDB 从 1.2.x 版本开始支持全文索引。Memory、NDB、Archive等不支持全文索引。\n\n  全文索引\n    \n    \n倒排索引技术全文索引通常使用倒排索引（Inverted Index）来实现：在辅助表（Auxiliary Table）中存储了单词与单词自身在一个&#x2F;多个文档中所在位置之间的映射，这通常利用关联数组实现，有两种表现形式：\n\nInverted File Index，表现形式为 {单词，单词所在文档 ID}；\nFull Inverted Index，表现形式为{单词，（单词所在文档 ID，在具体文档中的位置1，位置2…）}示例如下：\n\nInnoDB 全文索引采用 Full Inverted Index 的方式，将（DocumentId， Position1, Position2…）视为一个 ilist。故在全文索引的表中，有两个列：word字段、ilist字段，并且在word字段上设有索引。此外，由于 InnoDB 在ilist字段中存放了 Position 信息，故可以进行 Proximity Search（MyISAM不支持该特性）。为提高全文检索的并行性能，InnoDB中共有6张 Auxiliary Table，目前每张表根据 word 的 Latin 编码进行分区。Auxiliary Table 是存放在磁盘上的持久表。此外，还有一个 FTS Index Cache（全文检索索引缓存），用来提高全文检索的行能。它是一个红黑树结构，根据（word，ilist）进行排序。\n限制\n每张表只能有一个全文索引的索引；\n由多列组合而成的全文索引的索引列必须使用相同的字符集和排序规则；\n不支持没有单词界定符（delimiter）的语言，如中、日、韩等语言。\n\n相关性：在 WHERE 中使用 MATCH 函数，查询返回的结果是根据相关性（Relevance）进行降序排序的，即相关性最高的结果放在第一位，0表示没有任何相关性。计算条件：\n\nword是否在文档中出现；\nword在文档中出现的次数；\nword在索引列中的数量；\n多少个文档包含该word。\n\nBOOLEAN全文检索可以使用 IN BOOLEAN MODE 修饰符，此时查询字符串的前后字符会有特殊的含义，如：\n\n+ word：表示word必须存在；\n- word：表示word必须被排除；\n@distance：可选，表示字符字节间距最大值；\n&gt;：表示出现该词时增加相关性；\n&lt;：表示出现该词时降低相关性；\n*：表示以该词开头的单词，如lik*，可以是lik、like、likes…\n“：表示短语；\n~：表示允许出现该单词，但相关性为负\netc.\n\nQuery Expansion全文索引的扩展查询，通常在查询的关键词太短，用户需要 implied knowledge（隐含知识）时进行。如，对于单词 database 的查询，用户可能希望查询的不仅仅是 database 的文档，可能还指那些包含 MySQL、Oracle、DB2、RDBMS 等的单词，这时可以使用 Query Expansion 模式来开启全文索引的 implied knowledge。通过在查询短语中添加 WITH QUERY EXPANSION 或 IN NATURAL LANGUAGE MODE WITH QUERY EXPANSION 可以开启 blind query expansion（automatic relevance feedback）。该查询分为两个阶段：\n\n根据索引的单词进行全文索引查询；\n根据第一阶段产生的分词再进行一次全文检索的查询。\n\n由于 Query Expansion 的全文检索可能带来许多非相关性的查询，因此在使用时需要非常谨慎。    \n\n\n对比之所以InnoDB等数据库、数据库引擎选择B+而非其他类型的索引作为默认索引类型，除了上述B+索引的优势外，B+索引天然支持范围查找，这也是大部分关系型数据库的场景。\n索引创建哪些场景需要索引承前，索引的出现是为了解决（大数据量）情况下的查询效率问题。\n\n根据前面说的sql执行顺序，可以确定第一条是否添加索引的判断依据：该列数据是不是出现在where后。\n并非在所有的查询条件中出现的列都需要添加索引。对于添加 B+ 树索引的一般经验是，在访问表中很少一部分时考虑使用 B+ 树索引。如果一个字段取值范围很广，重复出现较少，即属于高选择性字段，此时使用 B+ 树索引比较合适。否则，如性别、类型等字段，可取值范围很小（性别显然只能筛选 50%），即属于低选择性字段，此时不适合使用 B+ 树索引（适合使用前面说的位图索引）。事实上，在低选择性的字段上，即使建立了如 B 树索引，数据库可能也不会使用，而是仍然以全扫描（如果没有其他可用索引）的方式进行查找，因为此时使用这种索引可能反而降低性能。\n\n哪些数据适合创建索引\n为用于搜索、排序、分组的列创建索引，而对于用作输出显示的列不需要建立索引。即，最佳索引候选列是出现在 WHERE、连接、ORDER BY、GROUP BY 子句中的列，而出现在 SELECT 关键字后面的输出列表里的列则不是很好的选择。\n认真考虑数据列基数列的基数（cardinality）是指它所容纳的所有非重复值的个数。相对于表的总行数来说，列的基数越大（也就是包含的唯一值越多，重复值越少，即上述的高选择性的列），使用索引的效果越好。\n选择合适的索引类型对于精确查找（使用 &#x3D;、&lt;&gt; 的查找），那么使用 MEMORY 的默认索引类型，即 hash 索引，可能效果更好：hash 的精确查找速度非常快，而对范围匹配表现欠佳；所以对于范围查找，使用 B+ 树类型的索引效果更好，InnoDB、MyISAM、MEMORY 等都支持 B 树索引。\n索引短小值应尽量选择较小的数据类型。如，当使用 MEDIUMINT 列便能够容纳所需要存储的数据时，就不要选用 BIGINT，如果值的长度都不会超过 25 个字符，那么就不要使用 CHAR(100)。短小的值可以提高索引的处理性能：\n短小的值操作更快，从而加快索引查找速度；短小的值可以让索引更小，从而减少磁盘 I&#x2F;O；对于短小的键值，键缓存里的索引块可以容纳更多的键值，也就可以在更少的磁盘 I&#x2F;O 下读取更多的索引块，从而提高找到键值的几率。\n\n\n索引字符串值的前缀要对字符串列建立索引，应该尽可能指定前缀长度。如，对于一个 CHAR(200) 列，如果大多数值的前 10 个 或 20 个字符都是唯一的，那么就可以不用为整个列进行索引，而只为前 10 个或 20 个字符进行索引，这样可以节省大量的索引空间，而且能够加速查询；但只索引列的第一个字符恐怕不行，因为会导致索引无法获得大量的唯一值。\n不要建立过多的索引显然，索引的建立并非无代价的，要考虑对性能和存储的影响。过多的索引也会导致\n\n辅助手段为数据选择利于高效查询的数据类型。多用数字运算，少用字符串运算数字运算通常比字符串运算更快。尽可能使用数字表示数据，如以点记号表示的 IP 地址，可以采用 4 组数字依次存入 INT UNSIGNED，而非直接使用字符串，尽管更方便，但数字对数据操作更高效，且更加节省空间。把数据列声明为 NOT NULL这可以加快查询，因为查询处理期间不再需要检查该列的值是否可以为 NULL，也有利于编写更简洁的 sql 语句。如果必须考虑 NULL 的情况，也可以考虑默认值等方式。考虑使用 ENUM 列如果必须采用字符串的列，恰好其基数很小（差异值较少），可以考虑转用 ENUM 列，即内部实际上采用数字形式存储，从而获得数字运算的处理速度。使用 PROCEDURE ANALYSE()运行 PROCEDURE ANALYSE()，可以根据输出得出一些优化手段。整理表碎片对频繁修改的表，尤其是包含可变长数据列的表，往往会产生大量碎片，导致空间浪费。定期使用 OPTIMIZE TABLE，可以消除&#x2F;减少碎片化的 InnoDB、MyISAM 表的空间浪费，并有助于防止性能降低。适用于各存储引擎的碎片整理方式是：先用 mysqldump 转储表，再利用该转储文件重建之：\nmysqldump db_name table_name > dump.sql\nmysql db_name &lt; dump.sql\n压缩数据把数据压缩到 BLOB 或 TEXT 列。使用 BLOB 或 TEXT 列来存储那些可以在应用程序中对其进行压缩和解压的数据，使之能够使用单个索引操作找出所有内容的目的。这种方法特别适用于存储那些难以用标准表数据结构表示的数据，或者会随时间变化的数据。把 BLOB 或 TEXT 列剥离处出来形成一个单独的表将 BLOB 或 TEXT 列单独存入一个附表，可以更好地管理原表里的数据\n如何使用索引一个索引是否适合某个查询的“三星系统”\n\n索引将相关的记录放到一起则获得一星；\n索引中的数据顺序和查找中的排练顺序一致则获得二星；\n索引中的列包含了查询中需要的全部列则获得三星。\n\n利用最左前缀当创建包含 n 个列的复合索引时，实际上会创建 n 个索引，即相当于多个索引，因为索引中最左边的任意数据列集合都可以用于匹配各个行。这样的集合即为 最左前缀。假设：多个列的复合索引中包含列：county、state、city，索引中行的排列顺序为 country&#x2F;state&#x2F;city。那么，行首先会按照 county&#x2F;state 顺序排序，然后按 country 顺序排序，这意味着，即使在查询中只指定了 county 值，或者只指定了 country 和 state 值，MySQL 也可以充分利用索引。因此，索引可用于搜索这些列组合：country, state, citycountry, statecountry对于没有包含最左前缀的搜索，如按照 state 或者 city 来搜索，MySQL 则无法使用该索引；而如果要所搜 country 和 city（索引的第 1、3 列，跳过第 2 列），那么索引能够找到与 country 匹配的行以缩小搜索范围，但无法继续用于 city 列。\n索引代价主要体现在索引导致的写操作效率降低和空间占用两方面。\n降低写操作效率写入操作（插入、删除、更新等）同时需要更新索引，所以表上的索引越多，需要做出的索引修改操作越多。所以在写操作频繁的表中，需要一些优化措施。\n占用空间索引会占用磁盘空间，尤其在多个索引的表中，索引可能导致很快达到表大小上限，这跟具体存储引擎有关：\n\nInnoDB 中，使用独立表空间的表，索引 + 数据行有文件大小限制，受操作系统影响；使用系统表空间则所有表和索引共享同一个存储空间池，添加索引会使存储空间减少得很快，但总大小不受操作系统的影响，逻辑上可以配置多个文件，所以可以使用磁盘扩容表空间。\nMyISAM 中，大量的索引可能导致索引文件比数据文件更快达到文件大小的上限。\n\nB&#x2F;B+树索引的限制\n如果不是按照索引的最左列开始查找，则无法使用该索引；\n不能跳过索引中的列。即，如果一个索引中含有三个列，则只有在使用了第一、二列的情况下，才能使用第三列；\n如果查询中有某个列的范围查询，则其右边所有列都无法使用索引查找。如果范围查询列值的数量有限，可以通过使用多个等于条件替代之。etc.\n\n索引存储\n对于 MyISAM 表，其数据保留在数据文件中，索引值保留在索引文件中，即使用数据的物理位置引用被索引的行，并使用前缀压缩技术使得索引更小，一个表可以有多个索引，但它们都保存在同一个索引文件里。索引文件里的每个索引都由一组有序的关键字行构成，这组关键字行主要用于快速访问数据文件；\n对于 InnoDB 表，默认情况下，只使用一个表空间，用于管理所有的 InnoDB 数据存储和索引存储，也可以修改配置，让它创建的每个表都有自己的表空间，但此时，给定表的数据和索引也同样保存在同一个表空间文件。同样地，InnoDB 将索引值当作一组有序值。InnoDB 存储引擎表是索引组织表，即表中数据按照主键顺序存放，即根据主键引用被索引的行。\n\n事务事务（Transaction） 是数据库区别于文件系统的重要特性之一。事务会把数据库从一种一致状态转变为另一种一致状态：提交工作时，可以确保所有修改要么都被保存好了，要么都不保存。\nACID\n原子性（Atomicity）：指数据库的事务是不可分割的工作单位，即事务中任何一个SQL语句执行失败，已经成功的部分也必须撤销，数据库会退到执行事务之前的状态；\n一致性（Consistency）：指事务将数据库从一种一致状态转变为下一种一致状态，在事务开始前和结束后，数据库的完整性约束没有被破坏，如列值的唯一性等；\n隔离性（Isolation）：也称并发控制（Concurrency Control）、可串行化（Serializability）、锁（Locking）等。要求每个读写事务的对象对其它事务的操作对象之间能够相互分离，即该事务提交前对其它事务都不可见，通常使用锁来实现。当前数据库系统都提供了一种粒度锁（Granular Lock）的策略，允许事务仅锁住一个实体对象的子集，以此来提高事务的并发度；\n持久性（Durability）：事务一旦提交，其结果就是永久性的，即使发生宕机等故障，数据库也能将数据恢复。持久性保证事务系统的高可靠性（High Reliability），但不能保证高可用性（High Availability）：_从事务本身的角度保证结果的永久性，但无法避免数据库本身的故障，如RAID卡损坏、自然灾害等_；\n\n_尽管理论上事务的定义需要满足这四个特性，但数据库厂商出于各种目的并不一定会完全遵循该准则_。\n事务分类从事务理论的角度来说，可以把事务分为以下几个类型：\n扁平事务（Flat Transactions）最简单的一种，也是实际生产环境中使用最为频繁的。所有操作都处于同一层次，由 BEGIN WORK 开始，由 COMMIT WORK &#x2F; ROLLBACK WORK 结束，其间的操作都是原子的——要么都执行，要么都回滚。是应用程序成为原子操作的基本组成模块。三种执行结果如下：\n\n\n带有保存点的扁平事务（Flat Transactions with Savepoints）允许事务执行过程中回滚到同一事务中较早的一个状态——某些事务可能在执行过程中出现的错误并不会导致所有的操作都无效，放弃整个事务不合乎要求，开销也太大。保存点（Savepoint）用来通知系统应该记住事务当前的状态，以便之后发生错误时，事务能回滚到保存点的状态。（扁平事务隐式在起始点设置一个保存点，即只能回滚到事务开始的状态）。保存点用 SAVE WORK 函数来建立。在发生系统崩溃时会丢失所有保存点，因为保存点是易失的（Volatile），而非持久的（Persistent），这意味着进行恢复时，事务需要从开始处重新执行而不能从最近的一个保存点继续执行。\n\n\n链事务（Chained Transactions）可以看作保存点模式的一个变种。在提交事务时，释放不需要的数据对象（如事务所持有的锁），将必要的处理上下文隐式地传给下一个要开始的事务（提交事务操作和开始下一个事务操作将合并为一个原子操作）。这意味着，下一个事务将看到上一个事务的结果，就好像在一个事务中进行的一样：\n\n链事务的回滚仅限于当前事务，即只能恢复到最近一个保存点。\n嵌套事务（Nested Transactions）层次结构框架，由一个顶层事务（top-level transaction）控制各个层次的事务，之下嵌套的事务称为子事务（subtransaction），其控制每一个局部的变换。\n\n嵌套事务的定义\n\n由若干事务组成的一棵树，子树既可以是嵌套事务，也可以是扁平事务；\n处在叶节点的事务是扁平事务，但每个子事务从根到叶节点的距离可以不同；\n位于根节点的事务称为顶层事务，其他事务为子事务。事务的前驱（predecessor）称为父事务（parent），下一层称为子事务（child）；\n子事务既可以提交也可以回滚，但它的提交操作并不会马上生效，除非其父事务已经提交。因此，任何子事务都在顶层事务提交后才真正的提交；\n树中的任意一个事务的回滚都会引起它的所有子事务一同回滚，因此，子事务仅保留 A、C、I 特性，不具有 D 的特性。\n\n在Moss的理论中，实际的工作是交由叶子节点完成的，即只有叶子节点的事务才能才能访问数据库、发送信息、获取其他类型的资源。而高层的事务仅负责逻辑控制，决定何时调用相关的子事务。即使一个系统不支持嵌套事务，用户也可以通过保存点技术来模拟嵌套事务：\n\n\n分布式事务（Distributed Transactions）通常是一个在分布式环境下运行的扁平事务，因此需要根据数据所在位置访问网络中的不同节点。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）。假如一个用户在 ATM 机上进行银行的转账操作，例如持卡人从招商银行存储卡转账 10000 元到工商银行的存储卡。这种情况下，可以将 ATM 机视为节点 A，招商银行的后台数据库视为节点 B，工商银行的后台数据库视为 C，这个转账的操作可分解为以下的步骤：\n\n\n节点 A 发出转账命令\n节点 B 执行存储卡中的余额减去 10000\n节点 C 执行存储卡终端的余额增加 10000\n节点 A 通知用户操作完成或者节点 A 通知用户操作失败\n\n\n这里需要使用到分布式事务，因为节点 A 不能通过一台数据库就完成任务，其需要访问网络中两个节点的数据库，而在每个节点的数据库执行的实务操作又都是扁平的，对于分布式事务，其同样需要满足 ACID 特性，要么都发生，要么都失效。对于上述例子，如果 2、3 步中任何一个操作失败，都会导致整个分布式事务回滚，若非这样，结果不可预知。\nInnoDB 支持的事务类型InnoDB存储引擎支持扁平事务、带有保存点的事务、链事务、分布式事务。MySQL数据库、InnoDB存储引擎都不原生支持嵌套事务，可通过带有保存点的扁平事务来模拟串行的嵌套事务。注意，使用分布式事务时，InnoDB 的事务隔离级别必须设置为 SERIALIZABLE。\n事务的实现事务的原子性、一致性、持久性通过数据库的 redo log 和 undo log 完成。redo log 称为重做日志，用来保证事务的原子性、持久性，undo log 用来保证事务的一致性。事务的隔离性由锁来实现。_undo 不是 redo 的逆过程_。redo 和 undo 都可以看作是一种恢复操作，redo 恢复提交事务修改的页操作，undo 回滚行记录到某个特定版本：两者记录的内容不同，并且 redo 通常是物理日志，记录的是页的物理修改操作，undo 是逻辑日志，根据每行记录进行记录。\nredo重做日志记录了事务的行为，由两部分组成：内存中的重做日志缓冲（redo log buffer），易失的；重做日志文件（redo log file），持久的物理保存。都是以512字节的块进行存储：重做日志块（redo log block）。通过 Force Log at Commit 机制实现事务的持久性，即当事务提交（commit）时，必须先将该事务的所有日志写入到重做日志文件进行持久化，待事务的commit操作完成才算完成。这里的日志分为两部分：redo log 和 undo log。redo log 基本是顺序写的，在数据库运行时不需要对 redo log 的文件进行读取操作，undo log 是需要进行随机读写的。为确保日志写入重做日志文件，重做日志缓冲写入文件系统缓存后，会进行一次fsync操作，以将缓冲写入磁盘文件，该同步操作的参数 innodb_flush_log_at_trx_commit 可以手动设置：\n\n1（默认）：事务提交时必须调用一次fsync操作；\n0：事务提交时不调用fsync，而是放到 master thread 中完成（默认每秒调用一次）；\n2：仅写入文件系统缓存，不进行fsync操作。宕机情况下会丢失未刷新到磁盘日志文件的那部分事务。\n\nundo对事务进行回滚操作。与redo存放在重做日志文件中不同，undo存放在数据库内部的一个特殊段（segment）中，这个段称为undo段（undo segment），位于共享表空间内。undo 是逻辑日志，只是将数据库逻辑地恢复到原来的样子，所有的修改都被逻辑地取消了：对于每个 INSERT，回滚执行一个 DELETE，相应的 DELETE、UPDATE也是同样执行一个相反的操作，将修改前的行放回去。除了回滚操作，另一个作用是完成 MVCC：当拥护读取一行记录时，若该记录已经被其它事务占用，当前事务可以通过undo读取之前的行版本信息，以此实现非锁定读。_uodo log 会产生 redo log，也就是 uodo log 的产生会伴随这 redo log 的产生，因为 undo log 也需要持久性的保护_。\nuodo log 格式InnoDB 中，undo log 分为：insert undo log：在 INSERT 操作中产生的 undo log。因为 INSERT 操作记录只对本事务可见（隔离性的要求），对其它事务不可见，所以可以在事务提交后直接删除；update undo log：在 DELETE、UPDATE 操作中产生的 undo log，该 undo log 需要提供 MVCC 机制，因此不能在事务提交时就删除，提交时放入 undo log 链表，等待 purge 线程进行最后的删除。\npurge用于最终完成 DELETE、UPDATE 操作。DELETE、UPDATE 操作可能不直接删除原有数据，可以仅将该行主键的 delete flag 设置为 1，行记录本身暂时不执行删除操作，而是延时到最终的 purge 操作中完成。这样设计是因为 InnoDB 支持 MVCC，所以记录不能在事务提交时立即进行处理。这时其它事务可能正在引用这行，故需要保存记录之前的版本。若该行记录已经不被任何其它事务引用，即可进行真正的删除操作。\ngroup commit为提高磁盘 fsync 操作的效率，数据库提供 group commit 功能使得一次 fsync 操作可以刷新多个事务日志到磁盘文件。\nXA 规范XA 是 X&#x2F;Open DTP 组织（X&#x2F;Open DTP group）定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供（MySQL、Oracle、SQL Server、DB2等数据库都本地支持）。在这个模型中，包括：\n\n应用程序（Application Program）：定义事务的边界，指定全局事务中的操作；\n事务管理器（Transaction Manager）：协调参与全局事务中的各个事务，需要和参与全局事务的所有资源管理器进行通信；\n资源管理器（Resource Managers）：提供访问事务资源的方法，通常一个数据库就是一个资源管理器。\n通信资源管理器（CRM）。\n\n两阶提交协议和三阶提交协议就是根据这一思想衍生出来的。可以说两阶段提交其实就是实现 XA 分布式事务的关键(两阶段提交主要保证了分布式事务的原子性)。\n两阶段提交协议two phase commit protocol，2PC。在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，两阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。\n所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。\n准备阶段事务管理器（事务协调者）给每个资源管理器（事务参与者）发送 Prepare 消息，每个参与者要么直接返回失败（如权限验证失败），要么在本地执行事务，并且写本地的 redo 和 undo 日志，但不提交，而是处于一种“准备好了”的状态：\n\n协调者向所有参与者询问是否可以执行提交操作（vote），并开始等待它们的响应；\n参与者执行询问发起为止的所有事务操作，并将 undo 和 redo 信息写入日志；\n各参与者响应协调者发起的询问，如果参与者成功执行事务操作，返回“同意”，否则返回“终止”。\n\n提交阶段如果协调者收到了（任一）参与者的失败消息或者超时，那么直接给每个参与者发送回滚（Rollback）消息；否则，发送提交（Commit）消息。参与者根据收到的指令执行回滚&#x2F;提交操作，随之释放所有事务处理过程中使用的锁资源(必须在最后阶段释放锁资源)。不管最后结果如何，第二阶段都会结束当前事务。\n提交\n如果进入提交阶段（所有参与者都返回的是“同意”消息）：\n\n协调者节点向所有参与者节点发出“正式提交(commit)”的请求。\n参与者节点正式完成操作，并释放在整个事务期间内占用的资源。\n参与者节点向协调者节点发送”完成”消息。\n协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。\n\n回滚\n如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：\n\n协调者节点向所有参与者节点发出“回滚操作(rollback)”的请求。\n参与者节点利用之前写入的 undo 信息执行回滚，并释放在整个事务期间内占用的资源。\n参与者节点向协调者节点发送“回滚完成”消息。\n协调者节点受到所有参与者节点反馈的“回滚完成”消息后，取消事务。\n\n容错\n协调者正常，参与方 crash若参与方在准备阶段 crash，则协调者收不到 Prepared 回复，协调方不会发送 commit 命令，事务不会真正提交。若参与方在提交阶段提交，当它恢复后可以通过从其它参与方或者协调方获取事务是否应该提交，并作出相应的响应。\n协调者 crash，参与者正常可以通过选出新的协调者解决。\n协调者和参与方都 crash两阶段提交无法完美解决。尤其是当协调者发送出 commit 命令后，唯一收到 commit 命令的参与者也 crash，此时其它参与方不能从协调者和已经 crash 的参与者那儿了解事务提交状态。但如同上一节两阶段提交前提条件所述，两阶段提交的前提条件之一是所有 crash 的节点最终都会恢复，所以当收到 commit 的参与方恢复后，其它节点可从它那里获取事务状态并作出相应操作。\n\n存在的问题\n同步阻塞问题执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。\n单点故障由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）。\n数据不一致在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送 commit 请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了 commit 请求。而在这部分参与者接到commit请求之后就会执行 commit 操作。但是其他部分未接到 commit 请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。\n无法解决的问题：协调者再发出 commit 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。\n\n三阶段提交协议three phase commit protocol，3PC。2PC 的改进版。3PC 主要解决的单点故障问题，并减少阻塞。\n[参考]http://www.hollischuang.com/archives/681\n锁锁是数据库系统区别于文件系统的一个关键特性。锁机制用于管理对共享资源的并发访问，提供数据的 完整性和一致性。多用户、数据库驱动的应用中，很大的一个难点是：一方面要最大程度地利用数据库的并发访问，一方面要确保每个用户能以一致的方式读取和修改数据。为此有了锁（locking）机制。\n不同数据库的锁实现机制一般不同MySQL 中，MyISAM 引擎使用表锁，并发读没有问题，但并发插入的性能就要差很多了。InnoDB 引擎提供一致性的非锁定读、行级锁支持。行级锁没有相关额外的开销，并可以同时得到并发性和一致性。\nMySQL 锁MySQL 中共有 4 种锁。\n\n表锁\n页锁\n行锁（少数独立存储引擎才有，如 InnoDB）\n元数据锁：5.5 版本加入的新特性。仅对表中的元数据启用，当有线程开始使用表时，元数据锁就会锁住表的所有元数据。\n\n元数据DDL 语句的更改信息，如 CREATE、DROP、ALTER 等修改方案的语句。老版本中引入元数据是为了解决线程可以在其他线程中的并发事务使用相同表的情况下修改表定义或是删除表的问题。\nlatch 和 lock这是锁中容易混淆的两个概念。两者都可以称为“锁”。\n\nlatch 一般称为闩锁（轻量级的锁），对象是线程，因为其要求锁定的时间必须非常短，若持续时间长，则应用的性能会非常差，InnoDB 中，latch 又分为 mutex（互斥量）和 rwlock （读写锁）。目的是用来保证并发线程操作临界资源的正确性，通常没有死锁检测的机制；\nlock 的对象是事务，用来锁定的是数据库中的对象，如表、页、行等，一般 lock 的对象仅在事务 commit&#x2F;rollback 后进行释放（不同事务隔离级别释放的时间可能不同），有死锁检测机制。\n\nInnoDB锁标准行级锁InnoDB 提供两种标准的行级锁：\n\n共享锁（S Lock）：允许事务读一行数据；\n排他锁（X Lock）：允许事务删除或更新一行数据；\n\n锁兼容（Lock Compatible）：如果一个事务 T1 已经获得了行 r 的共享锁，那么另外的事务 T2 可以立即获得行 r 的共享锁，因为读取并不会改变行 r 的数据，这种情况就是锁兼容。S 和 X 都是行锁，兼容是指对同一行记录（row）锁的兼容性情况。\n行级锁优缺点InnoDB 行锁效率很高，占用内存也很少，但细粒度的行锁在锁定的时候仍然会带来额外开销，比表锁、页锁消耗的内存要多；细粒度意味着锁的请求数量可能较多，而较多的锁会占用资源，降低性能；在执行对大部分数据的 GROUP BY 操作或频繁扫描很多数据时，性能会明显下降。\n意向锁（表级）InnoDB 支持多粒度（granular）锁定，允许 事务在行级上的锁和表级上的锁同时存在。为此，InnoDB 支持一种额外的 表级别锁——意向锁。可以理解为意向锁将锁定的对象分为多个层次，意味着事务希望在更细粒度（fine granularity）上进行加锁，先要在粗粒度上加相应的意向锁，以表明在接下来的动作想要获取该记录的锁。如果将上锁的对象看成一棵树，那么对最下层的对象上锁，也就是对最细粒度的对象进行上锁，则首先需要对粗粒度的对象上锁。如，对页上的记录 r 上 X 锁，那么分别要对数据库A、对应表、对应页上意向锁 IX，最后对记录 r 上 X 锁。如果其中任何一个部分导致等待，那么该操作需要等待粗粒度锁的完成才能继续。\n\n意向锁作为表级锁，其设计目的是为了在一个事务中揭示下一行将被请求的锁类型。意向锁的枷锁过程是自动的，无需用户干预。InnoDB 支持两种意向锁：\n\n意向共享锁（IS Lock）：事务想要获得一张表中某几行的共享锁；\n意向排他锁（IX Lock）：事务想要获得一张表中某几行的排他锁。\n\n由于 InnoDB 支持的是行级别的锁，因此意向锁不会阻塞除全表扫描以外的任何请求。锁的兼容性如下：\n\n\n一致性非锁定读（Consistent Nonlocking Read）指 InnoDB 通过行多版本控制（multi versioning）的方式来读取当前执行时间数据库中行的数据。如果读取的行正在执行 DELETE 或 UPDATE 操作，这时读取操作不会因此去等待行上的锁释放。而是，InnoDB 会去读取该行的一个快照数据：\n\n之所以称之为非锁定读，因为不需要等待访问的行上 X 锁的释放。快照数据是指该行的之前版本的数据，一个行记录可能有不止一个快照数据，该实现是通过 undo 段来完成，而 undo 用来在事务中回滚数据，因此快照数据本身没有额外开销。此外，读取快照数据不需要上锁，因为没有事务需要对历史数据进行修改操作。非锁定读机制极大提高了数据库的并发性，这是 InnoDB 默认设置下的读取方式。但在不同事务隔离级别下，读取的方式不同：\n\nREAD COMMITTED：总是读取行的最新版本，锁定则读最新快照版本；\nREPEATABLE：总是读取事务开始时的行数据。\n\n一致性锁定读对于增删改语句，InnoDB 会自动给涉及的数据集加排他锁，而对普通的 SELECT 语句则不会加任何锁。某些情况下如典型的事务、存储过程等，用户需要显式地对数据库读取操作加锁，以保证数据逻辑的一致性。InnoDB 对于 SELECT 语句支持两种一致性的锁定读（Locking Read）操作：\n\nSELECT…FOR UPDATE：对读取的行记录加一个 X 锁，\nSELECT…LOCK IN SHARE MODE：对读取的行记录加一个 S 锁。\n\n上述语句必须在一个事务中，当事务提交了，锁也就释放了。因此要加上  BEGIN，START TRANSACTION 或 SET AUTOCOMMIT&#x3D;0。\n外键和锁InnoDB 中，如果外键列没有显式添加索引，则自动对其添加一个索引，以避免表锁。\n锁算法InnoDB 3 种行锁算法：\n\nRecord Lock：单个行记录上的锁，总是锁住索引记录，如果表上没有索引，则使用隐式的主键进行锁定；\nGap Lock：间隙锁，锁定一个范围，但不包含记录本身；\nNext-Key Lock：Record Lock + Gap Lock，锁定一个范围，并锁定记录本身。能够解决幻读问题。当查询的索引含有唯一属性时，InnoDB 会将其优化降级为 Record Lock，即仅锁住索引本身而不再是范围。\n\nNext-Key 锁当我们用范围条件（而非相等条件）检索数据并请求锁时，InnoDB 会给符合条件的已有数据记录项加锁，对于键值在条件范围内但并不存在的记录，叫做“间隙（Gap）”，InnoDB 也会对这个间隙加锁，这就是所谓的 Next-Key 锁机制。如，emp 表有 101 条记录，其 empid 值分别为 1、2、3…101，则以下语句：\nSELECT * FROM emp WHERE empid > 100 FOR UPDATE;\n这条查询语句就是范围检索，InnoDB 不仅会对符合条件的 empid 记录，这里只有 101 一行加锁，也会对 empid &gt; 100 的“间隙”加锁，尽管并不存在这样的数据，但考虑这样一个场景，如果其他事务试图插入 empid &gt; 100 的数据，此时就不会成功，因为该范围被锁定，插入操作会失败。而如果不采取该锁机制，则插入有可能会成功，就会出现幻读现象。显然，这种锁机制会阻塞符合条件查询内键值的并发插入，这往往造成严重的锁等待和性能问题，因此，在并发场景下，应该考虑优化业务逻辑，尽量使用相等条件进行检索，避免范围检索。但要注意一个问题：如果使用相等条件查询，而查询结果不存在这样的记录，那么 InnoDB 会使用 Next-Key 锁，这就意味着，几乎将全部的范围都锁定了。\n_事务隔离级别为 READ COMMITTED 时，仅采用 Record Lock_。\n锁导致的问题因为事务隔离性的要求，锁只会带来三种问题：\n\n脏读\n不可重复读\n丢失更新\n\n阻塞指由于不同锁之间的兼容性关系，在有些时刻一个事务中的锁需要等待另一个事务中的锁释放它所占用的资源。阻塞并不是一件坏事，是为了确保事务可以并发且正常执行。超时会跑出1205错误。调整参数↓innodb_lock_wait_timeout：设置等待时间（默认50秒），可动态调整；innodb_rollback_on_timeout：设置是否在等待超时时对进行中的事务进行回滚操作（默认OFF，不回滚），静态的，不可在启动时修改。\n死锁解决死锁最简单的方式是不要有等待，将任何的等待都转化为回滚，并且重新开始事务。然而这将导致并发性能的下降甚至所有的事务都无法进行。另一种解决方法是超时，即当事务相互等待时，当其中一个事务等待时间超过设置的阈值，进行回滚操作，这时其它事务就有可能继续执行。InnoDB中超时设置参数：innodb_lock_wait_timeout（默认50秒）。超时机制虽然简单，但若超时的事务所占权重较大，如操作更新了很多行，占用了较多的 undo log，此时回滚该事务的时间相比于其它事务所占用的时间可能更多。因此除了超时机制外，当前数据库普遍采用 wait_for graph（等待图）的方式来进行死锁检测。较之超时的解决方案，这是一种更为主动的死锁检测方式。等待图要求数据库保存以下两种信息：\n\n锁的信息链表；\n事务等待链表；\n\n通过上述链表构造出一张图，如果这个图中存在回路，就代表存在死锁，此时 InnoDB 选择一个 undo 量最小的事务进行回滚，并报告一个立即可见的错误，如果不能解除死锁，则继续选择回滚，直到死锁解除。等待图的死锁检测通常采用深度优先的非递归算法实现（1.2.x版本之后，之前采用递归的深度优先算法实现）。\n乐观锁 vs 悲观锁互斥同步、非互斥同步，并发控制技术手段上的概念。乐观锁（Optimistic Lock）假设数据一般情况下不会发生并发冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测。如果冲突，则返回错误，让用户决定如何去做，这意味着乐观锁不能解决脏读问题，其实现算是一种 CAS 操作。其实现：\n\n使用数据版本记录机制实现。为数据增加一个版本标识，如“version”，每更新一次数据，则对标识进行一次累加操作。\n时间戳（timestamp）记录机制：在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果不一致则冲突。\n\n适用于多读少写的场景，可以提高吞吐率。\n悲观锁（Pessimistic Lock）假定数据会发生并发冲突，从而以预防冲突的方式屏蔽可能违反数据完整性的冲突操作。简言之就是每次读写数据都认为会被其他线程修改，所以都需要先上锁。java synchronized 就属于悲观锁的一种实现。\n存储引擎MySQL 体系结构如图所示：由上图可以看出，MySQL由以下几部分组成：\n\n连接池组件；\n管理服务和工具组件；\nSQL 接口组件；\n查询分析器组件；\n优化器组件；\n缓冲（Cache）组件；\n插件式存储引擎；\n物理文件；\n\n其中，MySQL 数据库区别于其他数据库的最重要的一个特点就是其插件式的表存储引擎。并且为这种架构提供了一系列标准的管理和服务支持，这些标准与存储引擎本身无关，作为底层物理结构的实现，每个存储引擎按照自己的意愿进行开发。MySQL 的核心就在于存储引擎。存储引擎是基于表的，而不是数据库。\n常用存储引擎对比可以看出，不同存储引擎对诸如事务、全文索引等的支持是不同的；另外对B-树等索引、锁机制等的具体算法实现也有很大差异。\n选取 MySQL 中较为常用的 InnoDB、MyISAM、MEMORY、Archive 四个存储引擎作为对比。它们都有各自适用的环境，这取决于其独有的一些特征。主要体现在性能、事务、并发控制、参照完整性、缓存、 故障恢复，备份及回存等几个方面。\nInnoDB\nMySQL 5.7.x 默认的存储引擎为 InnoDB，被设计成适用于高并发读写的情况。使用MVCC(Multi-Version Concurrency Control)以及行级锁来提供遵从ACID的事务支持，提供了具有__提交、回滚和崩溃恢复能力的事务安全__，对比MyISAM的存储引擎，InnoDB 写处理效率差一些并且会占用更多的磁盘空间以保留数据和索引。\n提供外键约束（MySQL 中只有 InnoDB 支持外键约束）。在创建外键的时候，父表必须有对应的索引，子表在创建外键的时候也会自动创建对应的索引。在创建索引的时候，可以指定在删除、更新父表时，对子表进行的相应操作，包括restrict、cascade、set null和no action。其中restrict和no action相同，是指限制在子表有关联的情况下，父表不能更新；casecade表示父表在更新或删除时，更新或者删除子表对应的记录；set null 则表示父表在更新或者删除的时候，子表对应的字段被set null。\n在行级别上对表数据上锁，提供非锁定读。行级锁没有相关额外的开销，并可以同时得到并发性和一致性。\n使用 Next-Key Lock 算法来避免不可重复读问题。MySQL 文档中将不可重复读问题定义为（Phantom Problem），即幻读问题。\n\n适合场景：\n\n可靠性要求较高\n要求事务\n表更新和查询都相当频繁，锁定表的可能性较大，等\n\nMyISAM不支持事务，也不支持外键。访问速度快，占用内存较少。数据文件和索引文件可以分开存储，平均分配IO以获取更快的速度。每个MyISAM表在磁盘上存储3个文件：\n\n.frm（存储表定义）\nMYD（MYData，存储数据）\nMYI（MYIndex，存储索引）\n\nMyISAM表支持3种不同的存储格式：\n\n静态(固定长度)表\n动态表\n压缩表\n\n其中静态表是默认的存储格式。静态表中的字段都是非变长字段，这样每个记录都是固定长度的，这种存储方式的优点是存储非常迅速，容易缓存，出现故障容易恢复；缺点是占用的空间通常比动态表多。静态表在数据存储时会根据列定义的宽度定义补足空格，但是在访问的时候并不会得到这些空格，这些空格在返回给应用之前已经去掉。同时需要注意：在某些情况下可能需要返回字段后的空格，而使用这种格式时后面到空格会被自动处理掉。动态表包含变长字段，记录不是固定长度的，这样存储的优点是占用空间较少，但是频繁到更新删除记录会产生碎片，需要定期执行OPTIMIZE TABLE语句或myisamchk -r命令来改善性能，并且出现故障的时候恢复相对比较困难。压缩表由myisamchk工具创建，占据非常小的空间，因为每条记录都是被单独压缩的，所以只有非常小的访问开支。\n适合场景：\n\n对事务完整性没有要求；\n插入不频繁、查询非常频繁；等\n\nMERGEMERGE存储引擎是一组MyISAM表的组合，这些MyISAM表结构必须完全相同，MERGE表中并没有数据，对MERGE类型的表可以进行查询、更新、删除的操作，这些操作实际上是对内部的MyISAM表进行操作。对于对MERGE表进行的插入操作，是根据INSERT_METHOD子句定义的插入的表，可以有3个不同的值，first和last值使得插入操作被相应的作用在第一个或最后一个表上，不定义这个子句或者为NO，表示不能对这个MERGE表进行插入操作。可以对MERGE表进行drop操作，这个操作只是删除MERGE表的定义，对内部的表没有任何影响。MERGE在磁盘上保留2个以MERGE表名开头文件：.frm文件存储表的定义；.MRG文件包含组合表的信息，包括MERGE表由哪些表组成，插入数据时的依据。可以通过修改.MRG文件来修改MERGE表，但是修改后要通过flush table刷新。\nMEMORYMEMORY使用存在内存中的内容来创建表。每个MEMORY表实际对应一个磁盘文件，格式是.frm。MEMORY类型的表访问非常快，因为它到数据是放在内存中的，并且默认使用HASH索引，但是一旦服务器关闭，表中的数据就会丢失，但表还会继续存在。默认情况下，MEMORY数据表使用散列索引，利用这种索引进行“相等比较”非常快，但是对“范围比较”的速度就慢多了。因此，散列索引值适合使用在”&#x3D;”和”&lt;&#x3D;&gt;”的操作符中，不适合使用在”&lt;”或”&gt;”操作符中，也同样不适合用在order by字句里。如果确实要使用”&lt;”或”&gt;”或betwen操作符，可以使用B-tree索引来加快速度。存储在MEMORY数据表里的数据行使用的是长度不变的格式，因此加快处理速度，这意味着不能使用BLOB和TEXT这样长度可变的数据类型。VARCHAR是一种可变长类型，但因为它在MySQL内部被当作长度固定不变的CHAR类型，所以可以使用。\n查询上述索引的作用中最关键的地方在于“优化查询速度”，那首先需要了解的是，一条sql语句，它的执行顺序是怎样的，才能更好地理解如何加速，以及背后的为什么能够加速。（比这个“首先”更首先需要了解的是，为什么要“优化”？无非一：数据量太大了；二：数据量太大了内存存不下而硬盘又太慢但又不得不放在硬盘上因为它很便宜。根因是数据量太大了，以至于不优化的话又回到了点个按钮可以喝杯咖啡再回来看结果的原始状态，用户就要爆炸，用户一爆炸老板就要爆炸，so…）sql语句执行可能会划分为多个操作步骤，如下图，一个典型的sql查询，总共分为11步，其中每一步都会产生一张中间状态的虚表，作为下一步的输入，最后一步的虚表作为返回结果，其他步骤的虚表对用户透明。其中，每一步的大致过程如下：\n\nFORM: 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1。\nON: 对虚表VT1进行ON筛选，只有那些符合&lt;join-condition&gt;的行才会被记录在虚表VT2中。\nJOIN： 如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3，如果from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止。\nWHERE： 对虚拟表VT3进行WHERE条件过滤。只有符合&lt;where-condition&gt;的记录才会被插入到虚拟表VT4中。\nGROUP BY: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5。\nCUBE | ROLLUP: 对表VT5进行cube或者rollup操作，产生表VT6。\nHAVING： 对虚拟表VT6应用having过滤，只有符合&lt;having-condition&gt;的记录才会被 插入到虚拟表VT7中。\nSELECT： 执行select操作，选择指定的列，插入到虚拟表VT8中。\nDISTINCT： 对VT8中的记录进行去重。产生虚拟表VT9。\nORDER BY: 将虚拟表VT9中的记录按照&lt;order_by_list&gt;进行排序操作，产生虚拟表VT10。\nLIMIT：取出指定行的记录，产生虚拟表VT11, 并将结果返回。\n\n理解上述步骤之后，不难发现，为什么连接查询很可能导致低效（笛卡尔积：m * n），同时也可以看出，如果想要做一些查询优化工作，那么就要从 on、where 开始着手（join 中 on 等同于 where），对过滤条件中的字段加以优化（即挑选索引时的考虑因素）。\nsql分析explain分析语法：explain + SQL语句执行explain分析语句的结果列中，有几列是与索引使用情况相关的：\n\ntype：join 类型，判断此次查询是全表扫描还是索引扫描等信息；\npossible_keys：指出MySQL能使用哪个索引在该表中找到行；\nkey：显示MySQL实际决定使用的键(索引)。如果没有选择索引,键是NULL；\nkey_len：显示MySQL决定使用的键长度。如果键是NULL,则长度为NULL；\nref：显示使用哪个列或常数与key一起从表中选择行。\n\n语句执行分析Explain 使用分析\n","categories":["db"],"tags":["db"]},{"title":"computer network","url":"/cs/cn/","content":"层次网络模型OSI(Open System Interconnect，开放式系统互联) 7 层模型：\n\n重要的内容集中在应用层（HTTP、FTP、SMTP、远程登陆协议（TELNET、Rlogin），运输层（TCP、UDP），网络层（IP、ICMP、IGMP、RIP）。\n应用层应用层协议HTTP HTTPs拥控算法CDN","categories":["cn"],"tags":["cn"]},{"title":"OS","url":"/cs/os/","content":"","categories":["os"],"tags":["os"]},{"title":"a question a day","url":"/cs/aqad/","content":"","categories":["aqad"],"tags":["aqad"]},{"title":"java","url":"/cs/java/","content":"类文件结构各种不同平台的 VM 与所有平台统一使用的程序存储格式——字节码（ByteCode）是构成平台无关性的基石。然而，VM 的目标显然不止于平台无关，其语言无关性也同样在发展：目前已经有 Groovy、Jython、JRuby、Scala 等语言同样能够运行在 JVM 上。而实现语言无关性的基础仍然是 VM 和字节码存储格式。JVM 不和包括 java 在内的任何语言绑定，它只和“class 文件”这种特定格式的二进制文件格式所关联，其中包含了 JVM 指令集和符号表以及若干其他辅助信息。任何其他语言同样可以使用合适的编译器将其代码编译为 class 文件从而在 JVM 上执行，JVM 本身不关心 class 文件来源于何种语言——只要该 class 文件符合 jvms 的语法和结构化约束即可。字节码所能提供的语义描述能力强于 java 语言。\nclass 类文件的结构&#x2F;字节码以 8-byte 为单位紧凑排列，大端字节序方式，不含任何分隔符，格式是严格定义的：数据项顺序、数量，字节序，哪个字节代表什么含义等等，都不允许改变。文件格式采用类似于 C 语言结构体的伪结构，只含有两种数据类型：无符号数和表。\n\n无符号数：基本数据类型，以 u1、u2、u4、u8 表示 1&#x2F;2&#x2F;4&#x2F;8 个字节的无符号数。可以用来描述数字、索引引用、数量值或按照 UTF-8 编码构成字符串值；\n表是由多个无符号数或其他表作为数据项构成的复合数据类型。所有表习惯性以“_info”结尾。用于描述有层次关系的复合结构的数据。整个 class 文件本质上就是一张表。数据项：\n\n无论是无符号数还是表，当需要描述同一类型但数量不定的多个数据时，经常使用一个前置的容量计数器加若干个连续的数据项的形式，这时称这一系列连续的某一类型的数据为某一类型的集合。\n魔数与 class 文件版本每个 class 文件前 4 个字节称为魔数（Magic Number），唯一作用是确定该文件是不是一个能被 JVM 接受的 class 文件。_很多文件存储标准中都是用魔数进行身份识别，如图片格式（gif、jpeg等都在头文件中存有魔数），魔数值可以由文件制定者自由选取，但要避免引起混淆_。使用魔数而非扩展名是基于安全性方面的考虑：扩展名可随意改动。class 文件的魔数值为：0xCAFEBABE（咖啡宝贝？），这个值在 java 还称作“Oak”语言的时候（1991年前后）就已经确定下来了。紧接着魔数的 4 个字节存储的是 class 文件的版本号：56 字节是次版本号（Minor Version），78 字节是主版本号（Major Version）。Java 的版本号是从 45 开始的（JDK 1.xxx 为 45.xxx，而 JDK 1.8 为 52.xxx），JDK 1.1 之后的每个 JDK 大版本发布时，主版本号向上 +1（JDK 1.01.1 使用了 45.045.3 的版本号），高版本的 JDK 能向下兼容以前版本的 class 文件，但不能运行以后版本的 class 文件，即使文件格式未发生任何变化，JVM 也必须拒绝执行超过其版本号的 class 文件。\n常量池（Constant Pool）紧接着主次版本号之后的是常量池入口，常量池可以理解为 class 文件中的资源仓库，是 class 文件结构中与其他项目关联最多的数据类型，也是占用 class 文件空间最大的数据项之一，也是 class 文件中第一个出现的表类型数据项目。与 java 语言习惯不一样的是，常量池的容量计数器是从 1 而不是 0 开始的：为了满足某些索引在特定情况下需要表达“不引用任何一个常量池项目”的含义时，可以将索引值置 0。故总数量为 计数器值 - 1。class 文件中只有常量池的容量计数器是从 1 开始的。常量池主要存放两大类常量：\n\n字面量（Literal）：接近于 java 语言层面的常量概念，如文本字符串、声明为 final 的常量值等；\n符号引用（Symbolic References）：属于编译原理方面的概念，包含以下 3 类常量：\n类和接口的全限定名（Fully Qualified Name）字段的名称和描述符（Descriptor）方法的名称和描述符\n\n\n\nclass 文件中不保存各个方法、字段的内存布局信息，当 JVM 运行时，需要从常量池获取对应的符号引用，再在类创建时或运行时解析、翻译到具体的内存地址中。\n常量池中每一项常量都是一个表。目前（JDK1.8)共有 14 种结构各异的表数据结构（有 3 种新增于 JDK1.7，为更好地支持动态语言调用）。\n这 14 种 表都有一个共同特点：起始位置的 u1 标志位，代表当前这个常量属于哪种常量类型。每种常量项目的结构表如下：\n访问标志（Access flags）常量池结束之后，紧接着的是 2 字节的访问标志，用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口；是否定义为 public 类型；是否定义为 abstract 类型；如果是类的话，是否被声明为 final 等。当前已使用了 8 位，其余位置 0：\n类索引 父类索引 接口索引集合类索引（this_class）、父类索引（super_class）是 2 字节，各自指向一个类型为 CONSTANT_Class_info 的类描述符常量；接口索引集合（interfaces）是一组 2 字节类型的数据的集合，入口是 u2 类型的接口计数器，如果该类没有实现任何接口，则置 0，后面的接口索引表不再占用任何字节。这 3 项数据用来确定这个类的继承关系：\n\n类索引：确定该类的全限定名；\n父类索引：确定该类的父类的全限定名（java 不允许多继承，故只有一个，除了 Object 类，其余都有父类）；\n接口索引集合：描述该类实现了哪些接口，按 implements（如果这个类本身是接口，则是 extends）语句后的接口顺序排列。\n\n字段表（field_info）集合用于描述接口&#x2F;类中声明的变量。字段包括类级变量和实例级变量，但不包括在方法内部声明的局部变量。一个字段可以包括的信息有：字段的作用域（public、private、protected），实例变量还是类变量（static），可变性（final），并发可见性（volatile，是否强制从主内存读写），可否被序列化（transient），字段数据类型（基本类型、对象、数组）、字段名称。字段表结构如下：\n其中，字段访问标志（access_flags） 与类中的 access_flags 类似：\n跟随在 access_flags 的是两项索引值：name_index 和 descriptor_index，它们是对常量池的引用，分别代表着字段的简单名称、字段和方法的描述符。\npackage io.neil.hust;\npublic class HelloWorld &#123;\n    private String str = \"Hello, World\";\n    public void sayHello(String str) &#123;\n        System.out.println(str);\n    &#125;\n&#125;\n\n\n全限定名：“io&#x2F;neil&#x2F;hust&#x2F;HelloWorld”是”HelloWorld”类的全限定名（仅仅是把类全名“io.neil.hust.HelloWorld”的“.”换成了“&#x2F;”）；\n简单名称：没有类型和参数修饰的方法或字段名称：sayHello()方法和 str 字段的简单名分别为“sayHello”和“str”；\n字段和方法的描述符：描述字段的数据类型、方法的参数列表（数量、类型、顺序）和返回值。其标识字符含义如下：\n\n字段表都包含的固定数据项目到descriptor_index为止，之后跟随一个 属性表(attribute_info)集合 用于存储一些额外的信息，字段都可以在属性表中描述零至多项的额外信息。\n字段表集合中不会列出从超类或者父接口中继承而来的字段，但有可能列出原本Java代码之中不存在的字段，譬如在内部类中为了保持对外部类的访问性，会自动添加指向外部类实例的字段。另外，在Java语言中字段是无法重载的，两个字段的数据类型、修饰符不管是否相同，都必须使用不一样的名称，但是对于字节码来讲，如果两个字段的描述符不一致，那字段重名就是合法的。\n方法表（method_info）集合class 文件格式对方法的描述和对字段的描述几乎完全一致，仅在访问标志和属性表集合的可选项中有所区别。方法表访问标志\n属性表（attribute_info）集合class 文件、字段表、方法表都可以携带自己的属性表集合，用以描述某些场景专有的信息。与 class 文件中其他数据项要求严格的顺序、长度和内容不同，属性表集合的限制稍微宽松：不要求各个属性表之间遵循严格顺序，只要不和已有属性名重复，任何自定义编译器都可以自定义属性信息。而 JVM 运行时会忽略它不认识的属性。为了能正确解析 class 文件，jvms 7 预定义了 21 项属性（目前的 jvms 10 为 26 项），这里以 jvms 10 为例。\n对每个属性，它的名称需要从常量池中引用一个 CONSTANT_UTF8_info 类型的常量来表示，而属性值的结构则完全是自定义的，只需要通过一个 u4 的长度属性说明所占用的位数即可：\n类加载JVM 把描述类的数据从 class 文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被 JVM 直接使用的 java 类型，这就是 JVM 的类加载机制。java 中，类型的加载、连接和初始化过程都是在程序运行期间完成的，这种策略为应用程序提供了高度的灵活性。\n类加载时机类从被加载到 JVM 内存中开始，到卸载出内存为止，整个生命周期包括：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）、卸载（Unloading）。其中，验证、准备、解析3部分统称为连接（Linking）：\n其中，加载、验证、准备、初始化、卸载这 5 个阶段的顺序是确定的（但并非串行，通常是交叉混合式进行的，只是开始执行的顺序是确定的），而解析在某些情况下可能在初始化阶段之后再开始：为了支持 java 的运行时绑定（也称动态绑定&#x2F;晚期绑定）。jvms 规定了 5 种情况下，才能对类立即进行“初始化”：\n\n遇到 new、getstatic、putstatic、invokestatic 这4条字节码指令时，如果类没有进行过初始化，则先触发其初始化。常见场景是：使用 new 关键字实例化对象、读取或设置一个类的静态字段（被final修饰、已在编译期把结果放入常量池的静态字段除外）、调用一个类的静态方法；\n使用 java.lang.reflect 包的方法对类进行反射调用时，如果类没有进行过初始化，则先触发其初始化；\n初始化一个类时，如果发现其父类没有进行过初始化，则先触发其父类的初始化；\nJVM 启动时，用户需要指定一个要执行的主类，JVM 会先初始化该主类；\n使用JDK 1.7+ 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果是 REF_getStatic、REF_putStatic、REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则先触发其初始化。\n\n类加载过程加载“加载”是“类加载（Class Loading）”的一个阶段，jvms 要求这一阶段需要完成以下工作：\n\n通过一个类的全限定名来获取定义此类的二进制字节流；\n将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构；\n在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口。\n\n该阶段用户应用程序可以通过自定义类加载器参与，类加载其余过程由 JVM 主导并控制。\n验证是为了确保 class 文件的字节流中包含的信息符合当前 JVM 的要求，并且不会危害 JVM 自身的安全。是连接阶段的第一步。主要包括：\n\n文件格式验证，魔数、版本、常量池中常量是否被支持、是否含有 utf-8 以外的编码格式…\n元数据验证，对字节码描述信息进行语义分析校验，可能包括：该类是否有父类；父类是否继承了不允许继承的类（final 修饰的类）；如果不是抽象类，是否实现了父类&#x2F;接口必须实现的所有方法；字段、方法是否与父类产生矛盾…\n字节码验证，最复杂的验证阶段：通过数据流和控制流分析，确保程序语义合法、合乎逻辑；\n符号引用验证，JVM 将符号引用转化为直接引用，该校验发生在解析阶段。\n\n准备正式为类变量分配内存并设置类变量初始值（一般就是零值），这些变量所使用的内存都在方法区中分配：仅包括类变量（static 修饰）而不包括实例变量，实例变量将在对象实例化时随着对象一起分配在 java 堆中。\n解析JVM 将常量池内的符号引用替换为直接引用的过程。解析阶段的两者含义：\n\n符号引用（Symbolic References）：以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到内存中。各种虚拟机实现的内存布局可以各不相同，但是它们能接受的符号引用必须都是一致的，因为符号引用的字面量形式明确定义在Java虚拟机规范的Class文件格式中。\n可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经在内存中存在。\n\n初始化（类初始化）类加载过程的最后一步。准备阶段已经为类变量赋了零值，这一步是执行类构造器()方法，执行static域代码（静态类变量显式赋值代码和静态代码块）：\n\n编译器会在将 .java 文件编译成 .class 文件时，收集所有类初始化代码和 static {} 域的代码，收集在一起成为 () 方法；\n子类初始化时会首先调用父类的 () 方法；\nJVM 会保证 () 方法的线程安全，保证同一时间只有一个线程执行；\n\n注意，这里仅指类的初始化，跟实例的初始化阶段不是一回事。\n类加载器类加载器（Class Loader）是 java 语言的一项创新。在类加载的第一阶段“加载”过程中，需要通过 类的全限定名 来获取定义此类的二进制字节流，完成这个操作的模块就是 类加载器。这一操作是在 JVM 外部实现的，以便让应用程序自己（开发者）决定如何获取所需的类。jvms 并没有指明类的二进制字节流要从一个 .class 文件获取，也没有指明从哪里获取、怎样获取。这种开放使得 java 在许多领域得到充分利用，目前有以下常用方式获取字节流：\n\n从 zip 包中读取：jar，ear，war 等；\n从网络中获取，最典型的应用就是 Applet；\n运行时计算生成，最典型的是动态代理技术，在 java.lang.reflect.Proxy 中，就是用了 ProxyGenerator.generateProxyClass 来为特定接口生成形式为 “*$Proxy” 的代理类的二进制字节流；\n由其他文件生成，最典型的 jsp 应用，由 jsp 文件生成对应的 Class 类，etc.\n\n分类从 JVM 角度看，只存在两种类加载器：\n\n启动类加载器（Bootstrap ClassLoader），JVM 自身的一部分，C++ 实现。负责将 \\lib 目录中或被 -Xbootclasspath 参数所指定的路径中的类库加载到 JVM 内存中，这些类库必须能够被 JVM 识别（仅按照文件名识别，如 tr.jar，名称不符合的类库不会被加载）。_如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可_。\n所有其他的类加载器，独立于 JVM，java 实现，全部继承自抽象类 java.lang.ClassLoader。包括下面的扩展类加载器、应用程序类加载器等。\n\n从开发人员角度看，绝大部分 java 程序会使用到以下 3 种系统提供的类加载器：\n\n启动类加载器（Bootstrap ClassLoader）；\n扩展类加载器（Extension ClassLoader）,这个加载器由 sun.misc.Launcher $ExtClassLoader 实现，它负责加载\\lib\\ext目录中的，或者被 java.ext.dirs 系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器；\n应用程序类加载器（Application ClassLoader）：这个类加载器由 sun.misc.Launcher $App-ClassLoader 实现。这个类加载器是 ClassLoader 中的 getSystemClassLoader() 的返回值（所以也称它为系统类加载器）。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器。如果应用程序中没有自定义类加载器，一般情况下它就是默认类加载器。\n\n双亲委派模型\n所谓的类加载器的双亲委派模型指的是 类加载器之间的层次关系。图中所示类加载器之间的层次关系，就是类加载器的双亲委派模型（Parents Delegation Model）。双亲委派模型要求除了顶层的启动类加载器之外，其余的类加载器都应有自己的父加载器。类加载器之间的父子关系一般不会以继承关系（Inheritance）的关系来实现，而是使用组合（Composition）关系来复用父加载器的代码。_双亲委派模型不是强制性约束模型，而是 java 设计者推荐给开发者的一种类加载器实现方式_。\n工作过程如果一个类收到了类加载的请求，它首先不会自己尝试加载这个类，而是把这个请求委派给父加载器去完成，每一个层次的类加载器都是如此。因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成该加载请求（其搜索内没有找到所需的类）时，子类加载器才会尝试自己去加载。即搜索是从顶层加载器到发出加载请求的加载器各自维护的搜索范围依次进行搜索的。实现双亲委派模型的代码集中在 java.lang.ClassLoader 的 loadClass() 中，具体流程：先检查是否已经加载过该类，若没有，则调用父加载器的 loadClass()，若父加载器为 null，则默认使用启动类加载器作为父加载器。如果父加载器加载失败，抛出 ClassNotFoundException 后，调用自身 findClass() 进行加载。\nprotected Class&lt;?> loadClass(String name, boolean resolve)\n    throws ClassNotFoundException &#123;\n    synchronized (getClassLoadingLock(name)) &#123;\n        // 检查是否已经加载过该类\n        Class&lt;?> c = findLoadedClass(name);\n        if (c == null) &#123;\n            long t0 = System.nanoTime();\n            try &#123;\n                if (parent != null) &#123;\n                    c = parent.loadClass(name, false);\n                &#125; else &#123;\n                    c = findBootstrapClassOrNull(name);\n                &#125;\n            &#125; catch (ClassNotFoundException e) &#123;\n                // ClassNotFoundException thrown if class not found\n                // from the non-null parent class loader\n            &#125;\n\n            if (c == null) &#123;\n                // If still not found, then invoke findClass in order\n                // to find the class.\n                long t1 = System.nanoTime();\n                c = findClass(name);\n\n                // this is the defining class loader; record the stats\n                PerfCounter.getParentDelegationTime().addTime(t1 - t0);\n                PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);\n                PerfCounter.getFindClasses().increment();\n            &#125;\n        &#125;\n        if (resolve) &#123;\n            resolveClass(c);\n        &#125;\n        return c;\n    &#125;\n&#125;\n\n意义使用双亲委派模型来组织类加载器之间的关系，好处之一是：java 类随着它的类加载器一起，具备了一种带有优先级的层次关系。例如 java.lang.Object 类，它存放在 rt.jar 中，无论哪个类加载器请求加载这个类，最终都是委派给处于模型顶层的启动类加载器进行加载，因此 Object 类在程序的各种类加载器环境中都是同一个类。相对地，如果不使用双亲委派模型，而是由各个类加载器自行加载请求类的话，如果开发者编写了一个名为 java.lang.Object 的类，并把它放在程序的 ClassPath 中，那么系统将会出现多个不同的 Object 类，这会导致 java 类型体系中最基础的行为也无法保证，应用程序也将变得非常混乱。内存模型中，判断一个对象是否为某个类型时，前提是比较对象与被比较对象都来自同一个类加载器，否则没有对比的意义——返回 false。\n破坏双亲委派模型上述已经提到，这只是一个推荐实现，而非强制约束，这就意味着有可能（有意&#x2F;无意）不被遵循。（详见《深入理解java虚拟机》）。\n实例化一个对象在可以被使用之前必须要被正确地初始化，一个Java对象的创建过程包括初始化和实例化两个阶段。在实例化一个对象时，JVM首先会检查相关类是否已经加载并初始化，如果没有，则JVM立即进行加载并调用类构造器完成类的初始化。在类初始化过程中或初始化完毕后，根据具体情况对类进行实例化。\n所谓的实例化，以下面一行代码为例：Demo demo = new Demo();这条语句的动作就是创建一个对象，也就是实例化，它包含四个动作：\n\n右边的new Demo，是以Demo类为模板，在堆空间里创建一个Demo类对象（也简称为Demo对象）；\n末尾的()意味着，在对象创建后，立即调用Demo类的构造函数，对刚生成的对象进行初始化；\n左边的Demo demo创建了一个Demo类引用变量，所谓Demo类引用，就是以后可以用来指向Demo对象的对象引用。\n=操作符使对象引用指向刚创建的那个Demo对象。\n\n对象创建时机和方法\n使用new关键字创建对象；\n使用Class类的newInstance方法(反射机制)  通过Java的反射机制使用Class类的newInstance方法来创建对象，事实上，这个newInstance方法调用无参的构造器创建对象：  Student stu1 = (Student)Class.forName(\"Student类全限定名\").newInstance();　\nStudent stu2 = Student.class.newInstance();\n使用Constructor类的newInstance方法(反射机制)  　java.lang.relect.Constructor类里也有一个newInstance方法可以创建对象，该方法和Class类中的newInstance方法很像，但相比之下，Constructor类的newInstance方法更加强大些，我们可以通过这个newInstance方法调用有参数的和私有的构造函数：  Constructor&lt;Student> constructor = Student.class.getConstructor(String.class);\nStudent stu3 = constructor.newInstance(\"name\");\n使用Clone方法创建对象  调用一个对象的clone方法，JVM会创建一个新的、一样的对象。用clone方法创建对象的过程中并不会调用任何构造函数，且被克隆对象必须实现Cloneable接口并实现其定义的clone方法（原型模式）。注意浅克隆 深克隆。\n使用(反)序列化机制创建对象  反序列化一个对象时，JVM会创建一个单独的对象，在此过程中，JVM并不会调用任何构造函数，类需要实现Serializable接口。\n\n当一个对象被创建，JVM会为其分配内存来存放对象的实例变量及其从父类继承过来的实例变量(即使这些从超类继承过来的实例变量有可能被隐藏也会被分配空间)。在为这些实例变量分配内存的同时，这些实例变量也会被赋予默认值(零值)。在内存分配完成之后，JVM就会开始对新创建的对象进行初始化，主要涉及三种执行对象初始化的结构：实例变量初始化、实例代码块初始化、构造函数初始化。\n初始化（实例化阶段）在定义&#x2F;声明实例变量的同时，还可以直接对实例变量进行赋值或者使用实例代码块对其进行赋值。如果以这两种方式为实例变量进行初始化，那么它们将在构造函数执行之前完成这些初始化操作。实际上，如果对实例变量直接赋值或者使用实例代码块赋值，编译器会将这些代码放到类的构造函数中去，放在对超类构造函数的调用语句之后，本类构造函数代码之前：\npublic class InstanceVariableInitializer &#123;  \n    private int i = 1;  \n    private int j = i + 1;  \n\n    public InstanceVariableInitializer(int var)&#123;\n        System.out.println(i);\n        System.out.println(j);\n        this.i = var;\n        System.out.println(i);\n        System.out.println(j);\n    &#125;\n    // 实例代码块\n    &#123;\n        j += 3; \n    &#125;\n\n    public static void main(String[] args) &#123;\n        new InstanceVariableInitializer(8);\n    &#125;\n&#125;\n/**\nOutput: \n    1\n    5\n    8\n    5\n */\n\n\n\n对象内存布局对象头64位：\n32位：\n实例数据对齐填充JMM内存模型：为了保证共享内存的正确性（可见性、有序性、原子性），内存模型定义了共享内存系统中多线程程序读写操作行为的规范。其解决并发问题主要采用两种方式：限制处理器优化和使用内存屏障。\nJMM定义了JVM在计算机内存中的工作方式，主要涉及到 多线程之间共享变量的可见性以及如何在需要的时候对共享变量进行同步。JVM是整个计算机虚拟模型，所以JMM是隶属于JVM的。\n高速缓存机制目前，CPU的运算速度比计算机存储设备有几个数量级的差距，所以现代计算机都有一层（三层）读写速度尽可能接近处理器的高速缓存来作为内存与处理器之间的缓冲：将运算需要使用到的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了。基于高速缓存的存储交互很好地解决了处理器与内存之间的矛盾，但也为计算机体系架构设计带来了更高的复杂度，因为引入了一个新的问题：缓存一致性（Cache Coherence）：在多处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主存（Main Memory），当多个处理器的运算任务都涉及到同一块主内存区域时，将可能导致各自的缓存数据不一致，此时，同步回主存的数据以谁为准呢？为解决这种不一致，需要各个处理器访问缓存时都遵循一些协议来保证读写操作的正确性。这类协议有 MSI、MESI、MOSI、Synapse、Firefly、Dragon Protocol 等。而 内存模型，即可以理解为在特定的操作协议下，对特定的内存&#x2F;高速缓存进行读写访问的过程抽象。不同架构的物理机器可以有不同的内存模型，jvm 拥有自己的内存模型。\n重排序问题为使得处理器内部运算单元尽量被充分利用，处理器可能会对输入代码进行乱序执行（Out-of-Order Execution）优化，处理器会在计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的，但 不保证程序中各个语句计算的先后顺序与输入代码中的顺序一致。因此，如果存在一个计算任务依赖另外一个计算任务的中间结果，那么其顺序性并不能靠代码的先后顺序来保证。与处理器的乱序执行优化类似，jvm的即时编译器也有类似的指令重排（Instruction Reorder）优化。\njvms定义的java内存模型（Java Memory Model，JMM），试图屏蔽掉各种硬件和OS的内存访问差异，以实现让java程序在各种平台下都能达到一致的内存访问效果。目前已臻至成熟。JMM规定了所有的变量都存储在主内存（Main Memory）中（此处的主内存与上述物理硬件的主内存名字一样，两者也可以互相类比，但此处仅是jvm内存的一部分）。每条线程还有自己的工作内存（Working Memory，可与前面讲的处理器高速缓存类比），线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的变量。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。\n这里所讲的主内存、工作内存，和java内存区域中的 Java 堆、栈、方法区等并不是同一个层次的内存划分，这两者基本上是没有关系的，如果两者一定要勉强对应起来，那从变量、主内存、工作内存的定义来看，主内存主要对应于java堆中的对象实例数据部分，而工作内存则对应于虚拟机栈中的部分区域。从更低层次上说，主内存就直接对应于物理硬件的内存，而为了获取更好的运行速度，虚拟机（甚至是硬件系统本身的优化措施）可能会让工作内存优先存储于寄存器和高速缓存中，因为程序运行时主要访问读写的是工作内存。\n内存间交互操作JMM 规定了以下 8 种操作，来完成主内存和工作内存之间具体的交互协议——即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存之类的实现细节：\n\nlock（锁定）：作用于主内存变量，它把一个变量标识为仅能被一条线程独占的状态；\nunlock（解锁）：作用于主内存变量，它把一个处于锁定状态的变量释放出来，释放后的变量可以被其他线程访问&#x2F;锁定；\nread（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便后续的 load 操作使用；\nload（载入）：作用于工作内存的变量，它把 read 操作从主内存得到的变量值放入工作内存的变量副本中；\nuse（使用）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作；\nassgin（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时将会执行这个操作；\nstore（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传回到主内存中，以便后续的 write 操作使用；\nwrite（写入）：作用于主内存变量，它把 store 操作从工作内存中得到的变量的值放入到主内存的变量中。如果要把一个变量从主内存复制到工作内存，那就要顺序地执行read和load操作，如果要把变量从工作内存同步回主内存，就要顺序地执行store和write操作。注意，JMM只要求上述两个操作必须按顺序执行，而没有保证是连续执行。也就是说，read与load之间、store与write之间是可插入其他指令的。\n\n此外，JMM还规定了在执行上述8种基本操作时必须满足如下规则：\n\n不允许 read 和 load、store 和 write 操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者从工作内存发起回写了但主内存不接受的情况出现；\n不允许一个线程丢弃它的最近的 assign 操作，即变量在工作内存中改变了之后必须把该变化同步回主内存；\n不允许一个线程无原因地（没有发生过任何 assign 操作）把数据从线程的工作内存同步回主内存中；\n一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load 或 assign）的变量。换句话说，就是对一个变量实施 use、store 操作之前，必须先执行过了 assign 和 load 操作。\n一个变量在同一个时刻只允许一条线程对其进行 lock 操作，但 lock 操作可以被同一条线程重复执行多次，多次执行 lock 后，只有执行相同次数的 unlock 操作，变量才会被解锁；\n如果对一个变量执行 lock 操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行 load 或 assign 操作初始化变量的值；\n如果一个变量事先没有被 lock 操作锁定，那就不允许对它执行 unlock 操作，也不允许去 unlock 一个被其他线程锁定住的变量；\n对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存中（执行 store、write 操作）。\n\n8种内存访问操作和上述规则限定，再加上对volatile关键字的一些特殊规定，就已经完全确定了java程序中哪些内存访问操作在并发下是安全的。以上定义严谨但繁琐，其有一个等效判断原则——先行发生原则，也同样能够确定一个访问在并发环境下是否安全。\nlong 和 double对于64位的数据类型（long 和 double），JMM规定：允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行，即允许虚拟机实现不保证64位数据类型的load、store、read、write 这4个操作的原子性——但同时强烈建议jvm保证其操作的原子性，所以目前多数商用jvm都选择把64位数据的读写操作作为原子操作对待来实现。\n原子性 可见性 有序性JMM是围绕在并发过程中如何处理原子性、可见性和有序性这3个特性而建立起来的。\n原子性 AtomicityJMM直接保证基本数据类型（64位的姑且也可以包含在内）的读写访问（read、load、assign、use、store、write）具备原子性。如果场景需要更大范围的原子性保证（经常发生），JMM提供lock、unlock操作来满足这种需求。jvm并没有把这两个操作直接暴露给用户，而是提供了更高层次的字节码指令：monitorenter 和 monitorexit来隐式使用这两个操作。这两个字节码指令反映到java代码中就是同步块——synchronized关键字，因此在synchronized块之间的操作也具备原子性。\n可见性 Visibility当一个线程修改了共享变量的值，其他线程能够立即得知这个修改。JMM通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性。无论普通变量还是 volatile 变量都是如此，不同在于：volatile 的特殊规则保证了新值能够立即同步到主内存并且每次使用前立即从主内存刷新，从而保证了多线程操作中变量的可见性。除了 volatile 之外，synchronized 和 final也能保证可见性。同步块的可见性由“对一个变量执行 unlock 操作之前，必须把它同步回主内存中（执行 store、write 操作）”这条规则获得的；而 final 关键字的可见性是指：被 final 修饰的字段在构造器中一旦初始化完成，并且构造器没有把“this”的引用传递出去（this 引用逃逸是危险操作，其他线程有可能通过这个引用访问到”初始化了一半”的对象），那么在其他线程中就能看见 final 字段的值，并且无须同步就能被线程正确访问。\n有序性 Ordering如果在本线程观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指“线程内表现为串行的语义（Within-Thread As-If-Serial Semantics）”，后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象。这是 java 程序中天然的有序性。java 语言提供了 volatile 和 synchronized 关键字来保证线程之间操作的有序性，volatile 本身就包含了禁止对指令重排序的语义，而 synchronized 由“一个变量在同一个时刻只允许一条线程对其进行 lock 操作”这条规则获得有序性保证——该规则决定了持有同一个锁的两个同步块只能串行地进入。\n可见，synchronized 关键字在需要以上3种特性的时候都可以作为一种解决方案，事实上也是如此：大部分并发控制操作都可以使用它来完成。但滥用以及本可以考虑其他更优的方式而仍然选择使用synchronized，也将导致性能和效率的不同程度降低。\nHappen-Before 原则先行发生原则，JMM 定义的两项操作之间的偏序关系，是判断数据是否存在竞争、线程是否安全的主要依据：如果操作 A 先行发生于操作 B，i.e. 操作 A 发生在操作 B 之前，那么操作 A 产生的影响（应该保证）能被操作 B 观察到，“影响”包括修改了共享内存中变量的值、发送了消息、调用了方法等。以下为 JMM 天然存在的 Happen-Before 关系，这些先行发生关系无须任何同步器协助就能正常执行，即在编码过程中直接使用，并且如果两个操作之间的关系不在下述范围之列，或者无法从下述条目推导得出，那么它们就没有顺序性保证，jvm 就可以对它们进行重排序：\n\n程序次序规则（Program Order Rule）：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作（更准确的说法是控制流顺序而不是代码顺序，因为有分支、循环等结构）；\n监视器锁规则（Monitor Lock Rule）：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。这里强调同一个锁，而“后面”是指时间上的先后顺序；\nvolatile 变量规则（Volatile Variable Rule）：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。这里的“后面”同样是指时间上的先后顺序；\n传递性（Transitivity）：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么可以得出结论：操作 A 先行发生于 操作 C。\n线程启动规则（Thread Start Rule）：Thread 对象的 start() 方法先行于此线程的每一个动作；\n线程终止规则（Thread Termination Rule）：线程中的所有操作都先行发生于对此线程的终止检测，可以通过 Thread.join()方法的结束、Thread.isAlive()的返回值等手段检测到线程是否已经终止；\n线程中断规则（Thread Interruption Rule）：对线程 interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 Thread.interrupted()方法检测到线程是否有中断发生；\n对象终结规则（Finalizer Rule）：一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize()方法的开始；可以根据以上规则判定一端代码是否可以保持线程安全，即是否需要采取线程安全的措施。\n\nref：Java内存模型是什么\nvolatile关键字volatile可以被认为是jvm提供的最轻量级的同步机制。volatile变量在各个线程的工作内存中不存在一致性问题：在各个线程的工作内存中，volatile变量也可以存在不一致的情况，但由于每次使用之前都要先刷新，执行引擎看不到不一致的情况，因此可以认为不存在一致性问题。当一个变量被定义为volatile后，它具备两个特性：\n保证此变量对所有线程的可见性这里的“可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。（回想普通变量，其值在线程间传递均需要通过主内存来完成，例如，线程 A 修改一个普通变量的值，然后向主内存进行回写，另外一条线程 B 在线程 A 回写完成了之后再从主内存进行读取操作，新变量值才会对线程 B 可见）。但可见性并不意味着线程安全，事实上，volatile 并不是线程安全的，它并不能保证操作符合原子性。所以在一些有依赖的运算中，仍然要通过加锁（使用 synchronized 或 java.util.concurrent 中的原子类）来保证原子性。\n禁止指令重排序优化普通变量仅保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不保证变量赋值操作的顺序与程序代码中的执行顺序一致。而在一个线程的方法执行过程中无法感知到这点，这也就是 JMM 描述的所谓“线程内表现为串行的语义”（Within-Thread As-If-Serial Semantics）。从硬件上讲，指令重排不是以指令顺序任意执行，而是CPU在正确处理依赖情况的前提下，可能会将没有依赖关系的指令（来自于代码汇编的字节码）不按程序中代码顺序分开发送给相应电路单元进行处理，并且保证在后续有对之前的操作结果有依赖的地方能够取到正确的值，这样从最终结果上看依然是像有序执行的结果；而有依赖关系的代码指令之间不能重排。volatile能够禁止指令的重排优化，通过汇编代码可见，有volatile修饰的变量在执行操作后，会多执行一句“lock addl $0x0，(%esp)”指令（把 ESP 寄存器的值加 0，显然是一个空操作），这相当于一道内存屏障使得指令重排序无法逾越，关键就在于其lock前缀，它的作用是使得本CPU的Cache写入内存，同时该写入动作会引起别的CPU或者别的内核无效化（Invalidate）其Cache，即相当于对Cache中的变量做了一次JMM中的“store和write”操作。所以通过这样一个空操作，可让volatile变量的修改对其他CPU立即可见。\nvolatile vs lock显然，volatile并不等同于安全的锁，它也无法在多线程中保证安全性，但在特定场景下依然能够提供“足够的安全性”且比锁（synchronized 关键字或 java.util.concurrent 包里面的锁）效率更高。但随着 jvm 对锁的各种优化策略的进步——锁消除、锁粗化等，已无法简单定量分析究竟孰优孰劣。而如果让 volatile 自身对比，那可以确定一个原则：volatile 变量读操作的性能消耗与普通变量几乎没有什么差别，但是写操作则可能会慢一些，因为它需要在本地代码中插入许多内存屏障指令来保证处理器不发生乱序执行。不过即便如此，大多数场景下volatile的总开销仍然要比锁低，我们在volatile与锁之中选择的唯一依据应该是volatile的语义能否满足使用场景的需求。使用场景（满足以下条件时使用）\n\n对变量的写操作不依赖变量的当前值或其他的变量，或者能确保只有单个线程更新变量值；\n该变量不会与其他状态变量一起纳入不变性条件中；\n在访问变量时不需要加锁。\n\n单例模式双重校验public class DoubleCheckedLock &#123;\n    // 修饰禁止重排序\n    private volatile static DoubleCheckedLock instance;\n\n    private DoubleCheckedLock() &#123;\n        //构造器必须私有  不然直接new就可以创建\n    &#125;\n\n    public static DoubleCheckedLock getInstance() &#123;\n        //第一次判断，假设会有好多线程，如果doubleLock没有被实例化，那么就会到下一步获取锁,只有一个能获取到\n        //如果已经实例化，那么直接返回，减少除了初始化时之外的所有锁获取等待过程\n        if (instance == null) &#123;\n            //同步\n            synchronized (DoubleCheckedLock.class) &#123;\n                //第二次判断是因为假设有两个线程A、B,两个同时通过了第一个if，然后A获取了锁，进入然后判断doubleLock是null，\n                // 他就实例化了doubleLock，然后他出了锁，这时候线程B经过等待A释放的锁，B获取锁了，\n                // 如果没有第二个判断，那么他还是会去new DoubleLock()，再创建一个实例，所以为了防止这种情况，需要第二次判断\n                if (instance == null) &#123;\n                    //下面这句代码其实分为三步：\n                    //1.开辟内存分配给这个对象\n                    //2.初始化对象\n                    //3.将内存地址赋给虚拟机栈内存中的doubleLock变量\n                    //注意上面这三步，第2步和第3步的顺序是随机的，这是计算机指令重排序的问题\n                    //假设有两个线程，其中一个线程执行下面这行代码，如果第三步先执行了，就会把没有初始化的内存赋值给doubleLock\n                    //然后恰好这时候有另一个线程执行了第一个判断if(doubleLock == null)，然后就会发现doubleLock指向了一个内存地址\n                    //这另一个线程就直接返回了这个没有初始化的内存，所以要防止第2步和第3步重排序\n                    instance = new DoubleCheckedLock();\n                &#125;\n            &#125;\n        &#125;\n        return instance;\n    &#125;\n&#125;\n\nsynchronized偏向锁假设当前场景只有一个线程，并没有线程竞争关系，不仅不存在多线程竞争，而且总是由同一线程多次获得，那么此时这个线程如果获取到锁资源，锁状态就是偏向锁，此时Synchronized修饰的锁对象会把当前线程的ID存入到MarkWord中，以后只要不发生竞争，这个对象就归该线程所有。\n轻量级锁如果一个对象虽然有多线程要加锁，但加锁的时间是错开的（也就是没有竞争），那么可以使用轻量级锁来优化。\n重量级锁重量级锁的使用场景就是多个线程竞争同一个锁资源，如果在尝试加轻量级锁的过程中，CAS操作无法成功，这时一种情况就是有其它线程为此对象加上了轻量级锁（有竞争），此时需要进行锁膨胀，将轻量级锁变为重量级锁。\n优化锁膨胀jdk1.6之前，synchronized 是重量级锁。jdk1.6引入锁膨胀&#x2F;锁升级，即：无锁 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁有了锁膨胀机制之后，synchronized 的状态就多了无锁、偏向锁以及轻量级锁，这时候在进行并发操作时，大部分的场景不再需要用户态到内核态的转换，从而大幅的提升 synchronized 的性能。\n锁消除JIT编译器在编译的时候，进行逃逸分析，分析synchronized锁对象是不是只可能被一个线程加锁，不存在其他线程来竞争加锁的情况。比如StringBuffer对象，属于一个局部变量，并且不会从该方法中逃逸出去，那么此时就可以使用锁消除（不加锁），替换为StringBuilder，来消除锁。\n锁粗化锁粗化是指，将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。比如for循环内对某个对象连续加锁。\n自适应自旋检测到重量级锁并进入阻塞状态的开销比较大。jdk1.4引入自旋锁，即当前线程先不进入阻塞状态，而是进行一定次数的忙循环，也就是所谓的自旋，来等待获取当前共享资源的锁，期间不让出CPU资源，如果获取失败，再进入阻塞。jdk1.6引入自适应自旋锁，所谓的自适应，即不再自旋固定的次数，而是一个动态的次数，一个实践中的规则是：自旋次数通常由前一次在同一个锁上的自旋时间及锁的拥有者的状态决定。如果线程【T1】自旋成功，自旋次数为17次，那么等到下一个线程【T2】自旋时，也会默认认为【T2】自旋17次；如果【T2】自旋了5次就成功了，那么此时自旋次数就会缩减到5次。\nvs\nJUCJUC包主要包括这么几个模块：\n\natomic: 原子性包，包含有AtomicBoolean、AtomicInteger、AtomicIntegerArray等原子变量类，持有它们各自的对应的类型变量value，而且被volatile关键字修饰，这样来保证每次一个线程要使用它都会拿到最新的值。\ncollections: 集合类包，主要提供线程安全的集合，包括ArrayList对应的CopyOnWriteArrayList，HashSet对应的CopyOnWriteArraySet，HashMap对应的ConcurrentHashMap等；\nlocks: 锁包，提供锁机制，相比synchronized关键字来进行同步锁，功能更加强大，它为锁提供了一个框架，该框架允许更灵活地使用锁。包含的实现类主要有：\nReentrantLock，独占锁，同一个时间点只能被一个线程锁获取到的锁；\nReentrantReadWriteLock，包括子类ReadLock和WriteLock，ReadLock是共享锁，WriteLock是独占锁；\nStampedLock，\nLockSupport，具备阻塞线程和解除阻塞线程的功能，并且不会引发死锁。\n\n\nexecutor: 执行器，Java线程池的顶级接口，但它只是一个执行线程的工具，真正的线程池接口是ExecutorService，里面包含的类有：\nScheduledExecutorService，解决那些需要任务重复执行的问题；\nScheduledThreadPoolExecutor，周期性任务调度的类实现；\n\n\ntools: 工具包，包含一些同步辅助工具类：\nCountDownLatch，闩锁，同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待；\nCyclicBarrier，栅栏，允许一组线程互相等待，直到到达某个公共屏障点，并且在释放等待线程后可以重用；\nSemaphore，信号量，计数信号量，它的本质是一个“共享锁“。信号量维护了一个信号量许可集。线程可以通过调用 acquire()来获取信号量的许可；当信号量中有可用的许可时，线程能获取该许可；否则线程必须等待，直到有可用的许可为止。线程可以通过release()来释放它所持有的信号量许可。https://blog.csdn.net/weixin_43888181/article/details/116546374\n\n\n\nJUC入门\nReentrantLockReentrantLock继承了Lock，有三个内部类，关系如下：ReentrantLock 类内部总共存在Sync、NonfairSync、FairSync三个类，NonfairSync与 FairSync类继承自 Sync类，Sync类继承自 AbstractQueuedSynchronizer抽象类。\nAQSAbstractQueuedSynchronizer，抽象队列同步器，使用原生java代码实现了并发访问控制语义，定义出一套多线程访问共享资源的同步器框架，是整个JUC包的基石，Lock、ReadWriteLock、CountDownLatch、CyclicBarrier、Semaphore、ThreadPoolExecutor等都是在其基础上实现的。并发控制的核心是锁的获取与释放，AQS核心思想是，如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用CLH队列的变体实现，将暂时获取不到锁的线程加入到队列中。\nCLH锁CLH(Craig, Landin, and Hagersten locks)是一种基于链表的可扩展、高性能、公平的自旋锁，能确保无饥饿性，提供先来先服务的公平性。申请线程只在本地变量上自旋，它不断轮询前驱的状态，如果发现前驱释放了锁就结束自旋。CLH队列中的节点QNode中含有一个locked字段，该字段若为true表示该线程需要获取锁，且不释放锁，为false表示线程释放了锁。节点之间是通过隐形的链表相连，之所以叫隐形的链表是因为这些节点之间没有明显的next指针，而是通过myPred所指向的节点的变化情况来影响myNode的行为。CLHLock上还有一个尾指针，始终指向队列的最后一个节点。\n框架\nprivate transient volatile Node head;\nprivate transient volatile Node tail;\nprivate volatile int state;\nstatic final class ExclusiveNode extends Node &#123; &#125;\nstatic final class SharedNode extends Node &#123; &#125;\nabstract static class Node &#123; &#125;\n\n//原子的设置当前同步器的状态\nprotected final boolean compareAndSetState(int expect, int update) &#123;\n    return unsafe.compareAndSwapInt(this, stateOffset, expect, update);\n&#125;\n\nReentrantLock详解从ReentrantLock的实现看AQS的原理及应用\nCASCompare And Swap，比较并替换，在硬件&#x2F;底层层面是这样，而在java等语言层面的表述是 Compare And Set，如典型的Unsafe类、JUC很多同步类等。CAS机制使用3个基本操作数：内存地址V，旧的预期值A，要更新的值B：更新一个变量的时候，只有当变量的预期值A和内存地址V当中的实际值相同时，才会将内存地址V对应的值修改为B，这个过程在硬件层面保证是原子性的。\n主要场景：Atomic系列类；JUC Lock系列类的底层实现；Synchronized升级成重量级锁之前，也会用到CAS；\n缺点：ABA问题；CPU开销较大，在高并发场景下，很多线程反复尝试更新某个变量，竞争过多，自旋会给CPU带来比较大的压力；并不保证代码块的原子性，它只能保证一个变量的原子性，无法保证多个，不过这个问题可以转用AtomicReference，把多个变量封装在一个对象里：举个例子\nProxy或称 Surrogate，中文“代理”，也作为一种设计模式，其解释为：为其他对象提供一种代理以控制对该对象的访问。可以简单理解成“中介”。代理模式可以在不修改被代理对象的基础上，通过扩展代理类，控制被代理对象的访问，也可以为代理类添加新的操作&#x2F;行为。这在 AOP 中经常见到。一般使用工厂模式实现代理，这样可以将代理包装等操作实现封装在工厂的实现中。要和“装饰者模式”区别开，装饰者模式是 为对象增加行为，代理模式是 控制对象的访问；同时和“适配器模式”的区别是：适配器会改变对象的接口，而代理使用相同的接口（业务接口不变）。主要分为静态代理和动态代理两种，二者在功能上没有区别，这个划分依据是字节码的创建时机，前者在程序运行前就已经存在代理类的字节码文件，代理类和目标类的关系在运行前就确定了；后者的源码是在程序运行期间由JVM通过反射等机制动态生成的，在运行前并不存在代理类的字节码文件。\njdk静态代理一般是代理对象包装被代理对象。所谓静态，是指代理对象和目标对象都实现了相同的接口。简单的例子：\n\n  业务接口\n\npublic interface UserService &#123;\n    // 增加一个用户\n    void addUser();\n&#125;\n\n\n\n\n  业务实现\n\npublic class UserServiceImpl implements UserService &#123;\n    public void addUser() &#123;\n        System.out.println(\"增加一个用户。。。\");\n    &#125;\n&#125;\n\n\n\n  静态代理类\n\npublic class UserServiceProxy implements UserService &#123;\n    private UserServiceImpl userImpl;\n \n    public UserServiceProxy(UserServiceImpl countImpl) &#123;\n        this.userImpl = countImpl;\n    &#125;\n \n    public void addUser() &#123;\n        System.out.println(\"代理类方法，进行增强。。。\");\n        System.out.println(\"事务开始。。。\");\n        // 调用委托类的方法;\n        userImpl.addUser();\n        System.out.println(\"处理结束。。。\");\n    &#125;\n&#125;\n\n在使用的时候，直接初始化一个proxy类，通过这个实例调用代理方法：\n\n\n\n\n  测试\n\npublic static void main(String[] args) &#123;\n    UserServiceImpl userImpl = new UserServiceImpl();\n    UserServiceProxy proxy = new UserServiceProxy(userImpl);\n    proxy.addUser();\n&#125;\n\n\njdk动态代理也称接口代理，使用JDK API，动态地在JVM中构建出一个代理对象。被代理类 A 需要实现业务接口，业务代理类 B 需要实现 InvocationHandler 接口：\n\n  动态代理类\n\npublic class ServiceInvocationHandler implements InvocationHandler &#123;\n \n    // 目标对象\n    private Object target;\n \n    public ServiceInvocationHandler(Object target) &#123;\n        super();\n        this.target = target;\n    &#125;\n \n    /**\n     * 创建代理实例\n     * @return\n     * @throws Throwable\n     */\n    public Object getProxy() throws Throwable &#123;\n        return Proxy.newProxyInstance(Thread.currentThread()\n                .getContextClassLoader(), this.target.getClass()\n                .getInterfaces(), this);\n    &#125;\n \n    /**\n     * 实现InvocationHandler接口方法\n     * 执行目标对象的方法，并进行增强\n     */\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;\n        Object result = null;\n        System.out.println(\"代理类方法，进行增强。。。\");\n        System.out.println(\"事务开始。。。\");\n        // 执行目标方法对象\n        result = method.invoke(target, args);\n        System.out.println(\"事务结束。。。\");\n        return result;\n    &#125;\n&#125;\n\n\n\n  测试\n\n    /**\n    * jdk动态代理会生成一个动态代理类，生成相应的字节码，然后通过ClassLoader加载字节码。\n    * 该实例继承了Proxy类，并实现了业务接口，在实现的方法里通过反射调用了InvocationHandler接口实现类\n    * 的invoke()回调方法。\n    */\npublic static void main(String[] args) throws Throwable &#123;\n    UserService userService = new UserServiceImpl();\n    ServiceInvocationHandler handler = new ServiceInvocationHandler(userService);\n    // 根据目标生成代理对象\n    UserService proxy = (UserService) handler.getProxy();\n    proxy.addUser();\n&#125;\n\n\nJDK动态代理的限制在于，被代理的对象必须实现一个或多个接口，这是java语言本身决定的：java只支持单继承，而代理类本身已经继承了Proxy类，因此只能通过实现(被代理对象也实现了的)接口的方式。\ncglib动态代理如果某个类原本就没有实现任何接口，可以考虑cglib代理——对应地，它通过继承被代理的类，并覆写方法的方式，这就意味着，cglib不能代理final修饰的类。\n\n  UserDao\n\npublic class UserDao &#123;\n\n    public void save() &#123;\n        System.out.println(\"保存数据\");\n    &#125;\n&#125;\n\n\n\n  代理对象\n\npublic class ProxyFactory implements MethodInterceptor&#123;\n\n    private Object target;//维护一个目标对象\n    public ProxyFactory(Object target) &#123;\n        this.target = target;\n    &#125;\n    \n    //为目标对象生成代理对象\n    public Object getProxyInstance() &#123;\n        //工具类\n        Enhancer en = new Enhancer();\n        //设置父类\n        en.setSuperclass(target.getClass());\n        //设置回调函数\n        en.setCallback(this);\n        //创建子类对象代理\n        return en.create();\n    &#125;\n\n    @Override\n    public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123;\n        System.out.println(\"开启事务\");\n        // 执行目标对象的方法\n        Object returnValue = method.invoke(target, args);\n        System.out.println(\"关闭事务\");\n        return null;\n    &#125;\n&#125;\n\n\n\n\n  测试\n\npublic void testCglibProxy()&#123;\n    //目标对象\n    UserDao target = new UserDao();\n    System.out.println(target.getClass());\n    //代理对象\n    UserDao proxy = (UserDao) new ProxyFactory(target).getProxyInstance();\n    System.out.println(proxy.getClass());\n    //执行代理对象方法\n    proxy.save();\n&#125;\n\n\n对比\n静态代理实现较简单，只要代理对象对目标对象进行包装，即可实现增强功能，但静态代理只能为一个目标对象服务，如果目标对象过多，则会产生很多代理类。\n静态代理在编译时产生class字节码文件，可以直接使用，效率高。\n动态代理必须实现InvocationHandler接口，通过反射代理方法，比较消耗系统性能，但可以减少代理类的数量，使用更灵活。\ncglib代理无需实现接口，通过生成类字节码实现代理，比反射稍快，不存在性能问题，但cglib会继承目标对象，重写方法，所以目标对象不能为final类。\n\nCOWNIOThreadLocalMapUnsafe","categories":["java"],"tags":["java"]},{"title":"algorithm","url":"/cs/algs/","content":"sort初级排序几种基础排序算法，时间复杂度为O(n^2)。\n冒泡排序冒泡排序是最简单基础的排序算法：\n\n比较相邻的元素。如果第一个比第二个大，就交换他们两个；\n对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数；\n针对所有的元素重复以上的步骤，除了最后一个；\n持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较；其时间复杂度为O(n^2)，在原地交换，不需要额外的空间开销。是一种稳定排序。C++实现：void BubbleSort(int *a, int length)\n&#123;\n    for (int i = 0; i &lt; length; ++i)\n        for (int j = 0; j &lt; length - 1 - i; ++j)\n            if (a[j] > a[j + 1])\n                swap(a[j], a[j + 1]);\n&#125;\n\n插入排序来源于整理扑克牌的排序算法：将每一个元素插入到已经有序的序列中合适的位置，即前向交换使之到达正确位置。\nvoid InsertionSort(int *a, int length)\n&#123;\n    for (int i = 1; i &lt; length; ++i)\n        for (int j = i; j > 0; --j)\n            if (a[j] > a[j - 1])\n                swap(a[j], a[j - 1]);\n&#125;\n\n选择排序每次（第 i 次：0..N-1）都选择出剩余元素的最小值，将其置换到 i 的位置上去。\nvoid SelectionSort(int *a, int length)\n&#123;\n    for (int i = 0; i &lt; length; ++i) &#123;\n        int min = i;  // 最小元素的索引\n        for (int j = i + 1; j &lt; length; ++j)\n            if (a[j] > a[min])\n                min = j;\n            swap(a[i], a[min]);\n    &#125;\n&#125;\n\n希尔排序思想：使数组中任意间隔为 h 的元素都是有序的。类似于插入排序，只不过插入排序是交换相邻（间隔为1）元素，而希尔排序是交换间隔为 h 的元素。\n\nvoid ShellSort(int *a, int length)\n&#123;\n    int h = 1;\n    while (h &lt; length / 3)\n        h = 3 * h;\n    while (h >= 1) &#123;\n        // 将数组变为h有序\n        for (int i = h; i &lt; length; ++) &#123;\n            // 将a[i]插入到a[i-h]，a[i-2*h]，a[i-3h]...中\n            for (int j = i; j >= h &amp;&amp; a[j] &lt; a[j - h]; j -= h)\n                swap(a[j], a[j - h]);\n        &#125;\n        h = h / 3;\n    &#125;\n&#125;\n\n快排采用分治策略，（两路快排）将一个序列分成两个子序列，独立排序。与归并排序互补：归并排序将序列分成两个部分分别排序，并将有序的子序列归并以将整个序列排序，递归调用发生在处理整个序列之前；而快排则是，当两个子序列都有序时，整个序列也就自然有序了，递归调用发生在处理整个序列之后。\nvoid sort(Comparable[] a, int lo, int hi) &#123;\n    if (lo >= hi) return;\n\n    int j = partition(a, lo, hi);\n    sort(a, lo, j - 1);\n    sort(a, j + 1, hi);\n&#125;\n\nint partition(Comparable[] a, int lo, int hi) &#123;\n    int i = lo, j = hi + 1;  // 左右扫描指针\n    Comparable v = a[lo];  // 切分元素，主元\n    while (true) &#123;\n        // 扫描左右，检查扫描是否结束并交换元素\n        while (less(a[++i], v)) if (i == hi) break;\n        while (less(v, a[--j])) if (j == lo) break;\n        if (i >= j) break;\n        exch(a, i, j);\n    &#125;\n    exch(a, lo, j);  // 将v=a[j]放入正确位置\n    return j;  // a[lo..j-1]&lt;=a[j]&lt;=a[j+1..hi]达成\n&#125;\n优点\n\n实现简单，适用于各种不同输入数据；\n原地排序，只需要 O(1) 的辅助空间；\n\n缺点实现中要避免低劣性能的影响：划分是否相对均衡，即主元的选取；\n优化\n\n小规模子序列（7 ~ 15）改用插入排序；\n三取样切分：使用子序列一小部分元素的中位数作主元，取样大小为 3 时效果较好；\n\n堆排两步操作：堆化、下沉（或上浮）。\npublic static void sort(Comparable[] pq) &#123;\n    int n = pq.length;\n    // 构造堆\n    // 从最下层一个非叶节点开始作为根节点，构造子堆\n    // 这样能够保证对该节点的父节点堆化时，下沉动作不会扩散到它的子节点\n    for (int k = n / 2; k >= 1; k--) &#123;\n        sink(pq, k, n);\n        show(pq);\n    &#125;\n\n    // 堆排序过程\n    // 依次下沉当前最大元素，保持堆\n    while (n > 1) &#123;\n        exch(pq, 1, n--);\n        sink(pq, 1, n);\n    &#125;\n&#125;\n\n// 下沉，使得以 k 位置为根节点，构造一个堆\nprivate static void sink(Comparable[] pq, int k, int n) &#123;\n    while (2 * k &lt;= n) &#123;\n        int j = 2 * k;\n        if (j &lt; n &amp;&amp; less(pq, j, j + 1)) j++;\n        if (less(pq, j, k)) break;\n        exch(pq, k, j);\n        k = j;\n    &#125;\n&#125;\n\n归并排序将已有序的子序列合并，得到完全有序的序列的过程，i.e. 先使子序列有序，再使序列段间有序。\n\n时间复杂度： O(NlogN) ;\n空间复杂度：辅助空间：O(N);\n稳定排序，常使用递归实现。\n\nvoid merge(Comparable[] a, int lo, int mid, int hi) &#123;\n    int i = lo, j = mid + 1;\n\n    for (int k = lo; k &lt;= hi; k++)\n        aux[k] = a[k];\n\n    for (int k = lo; k &lt;= hi; k++) &#123;\n        if (i > mid)\n            a[k] = aux[j++];\n        else if (j > hi)\n            a[k] = aux[i++];\n        else if (less(aux[j], aux[i]))\n            a[k] = aux[j++];\n        else\n            a[k] = aux[i++];\n    &#125;\n&#125;\n\n// 自顶向下递归：\nvoid sort(Comparable[] a, int lo, int hi) &#123;\n    if (lo >= hi) return;\n    int mid = lo + (hi - lo) / 2;\n    sort(a, lo, mid);\n    sort(a, mid + 1, hi);\n    merge(a, lo, mid, hi);\n&#125;\n\n// 自底向上循环：\nvoid sort(Comparable[] a) &#123;\n    int N = a.length;\n    aux = new Comparable[N];\n    for (int sz = 1; sz &lt; N; sz = sz + sz) &#123;\n        for (int lo = 0; lo &lt; N - sz; lo += sz + sz)\n            merge(a, lo, lo + sz - 1, Math.min(lo + +sz + sz - 1, N - 1));\n    &#125;\n&#125;\n\n优化\n\n小规模子序列（7 ~ 15）改用插入排序&#x2F;选择排序；\n测试子序列是否已经有序：a[mid] &lt;&#x3D; a[mid]，则这两个子序列无需调用接下来的 merge() ，直接拷贝即可；\n不将元素复制到辅助空间：将辅助空间也带入 sort()、merge() 方法，每次递归变换二者的位置，从而无需反复拷贝子序列到辅助空间，而是临时将辅助空间用于排序和归并。\n\n线性时间排序比较排序的时间复杂度下限 O(n*logn) 是确定的。在这篇博客里有各种比较排序的对比。还有一类非比较排序算法，适用于一些特定情况。这种特定情况一般是对集合的范围界定：当集合满足一定条件，可以不使用比较的方式实现排序，从而获得优于比较排序下限的时间复杂度：线性时间复杂度内完成排序。常见的线性时间复杂度排序算法有：\n\n计数排序（Counting Sort）\n基数排序（Radix Sort）\n桶排序（Bucket Sort）\n\n计数排序限制条件：取值范围在 [m, n] 之间的整数，wiki解释集合分布在 [0, 100] 时最适合使用计数排序。原理：对每一个输入元素x，确定出小于x的元素个数，有了这一信息，就可以把x直接放在它在最终输出数组的位置上，例如，如果有17个元素小于x，则x就是属于第18个输出位置。当几个元素相同是，方案要略作修改。时间复杂度：O(n)。空间复杂度：O(n)。这是一种稳定排序。伪代码：\nCOUNTING-SORT(A;B; k)\nlet C[0..k] be a new array\nfor i = 0 to k\n\tC[i] = 0;\nfor j = 1 to A.length\n\tC[A[j]] = C[A[j]] + 1\n// C[i] now contains the number of elements equal to i .\nfor i = 1 to k\n\tC[i] = C[i] + C[i-1]\n// C[i] now contains the number of elements less than or equal to i .\n for j = A.length downto 1\n \tB[C[A[j]]] = A[j]\n \tC[A[j]] = C[A[j]] - 1\n\n基数排序原理：以十进制数组n为例，k&#x3D;10，最大数字的位数是d。把元素从个位排好序，然后再从十位排好序，，，，一直到元素中最大数的最高位排好序，那么整个元素就排好序了。时间复杂度：O(d(n+r))。空间复杂度：O(n+r)。这是一种稳定排序。伪代码：\nRADIX-SORT(A, d)\nfor  i = 1 to d\n\tuse a stable sort  to sort array A on digit i.\n\n桶排序假定数据服从均匀分布，均匀独立分布在[0， 1)区间。假设有m个桶，即将区间划分为m个大小相同的子区间。将n个元素分别存放到相应的桶中，再对各个桶进行排序，如插入排序。最后遍历每个桶，按照次序列出所有元素即可。时间复杂度：平均为O(n)。空间复杂度：O(n)。伪代码：\nBUCKET-SORT(A)\nn = A.length\nlet B[0.. n-1] be a new array\nfor i = 0 to n-1\n\tmake B[i] an empty list\nfor i = 1 to n\n\tinsert A[i] into B[⌊A[i]⌋]\nfor i = 0 to n-1\n\tsort list B[i] with insertion sort\nconcatenate the lists B[0], B[i],...B[n-1] together in order.\n\n\ndp与分治相似：都是通过组合子问题的解来求解原问题。而，分治将问题划分为互不相交的子问题，递归地（思路上）求解子问题，再将它们的解组合起来，得到原问题的解；与之不同地，动态规划应用于子问题重叠的情况，i.e. 不同的子问题具有公共的子子问题（递归求解子问题，将其划分为更小的子子问题）。这种情况下，分治算法将做许多不必要的工作——反复求解公共子子问题；而动态规划对每个子问题只求解一次，将结果保存在一个表格（programming 的由来）中，从而避免重复计算子子问题，i.e. 剪枝。典型的时空权衡（time-memory trade-off）问题：以空间换时间。通常用来求解 最优化问题（Optimization Problem），即在很多可行解中寻找 （一个&#x2F;多个）最优解 的过程。\n设计步骤\n刻画一个最优解的结构特征；\n递归地定义最优解的值；\n计算最优解的值——同常采用自底向上的方法；\n利用计算的信息构造一个最优解。\n\n两种实现方法\n\n带备忘录的自顶向下方法（top-down with memoization）。用数组&#x2F;散列表保存递归过程中子问题的解，当要求解一个子问题时，先检查是否已保存过此解，无则计算之；\n自底向上方法（bottom-up method）。需要恰当定义子问题“规模”的概念，使得任何子问题的求解都只依赖于“更小的”子问题的解。因而可以将子问题按规模排序，从小到大进行求解，当求解某个子问题时，它所依赖的更小的子问题已经求解完毕，结果已经保存。这样可以保证每个子问题只求解一次，并且求解它时，它所有的前提子问题都已完成。\n\n原理和适用条件适用场景：\n\n最优子结构；\n子问题重叠；\n\n贪心算法（greedy algorithm）每一步都做出当前看起来最佳的选择。即，总是做出 局部最优的选择。理论上，适用于贪心算法的问题，同样能用动态规划方法解决。\n设计步骤首先是基于动态规划的设计方式：\n\n确定问题的最优子结构；\n设计一个递归算法；\n证明如果做出一个贪心的选择，则只剩下一个子问题；并且，证明贪心选择总是安全的；\n设计一个递归算法实现贪心策略；\n将递归算法转换为迭代算法。\n\n更一般地，通过贪心选择改进最优子结构，使得选择后只剩下一个子问题：\n\n最优化问题转换为：做出一次选择后，只剩下一个子问题需要求解；\n证明做出贪心选择后，原问题总是存在最优解，即贪心选择总是安全的（没有因为这一次的选择而丢失客观的最优解）；\n证明做出贪心选择后，剩余的子问题满足性质：其最优解与贪心选择组合即可得到原问题的最优解，即得到了最优子结构。\n\n能否使用贪心的两个要素贪心选择性质（greedy-choice property）通过做出局部最优（贪心）选择来构造全局最优解。与动态规划不同，贪心算法进行选择时可能依赖之前做出的选择，但不依赖任何将来的选择或子问题的解。\n最优子结构同动态规划。\n与动态规划的差别考虑 0-1 背包问题和分数背包问题，显然，后者能够使用贪心算法解决，而 0-1 背包问题则只能使用动态规划。\nKMP算法经典的字符串匹配算法。但实现起来并不复杂。首先一个概念是：\n部分匹配表 The Partial Match Table参考这篇博文：The Knuth-Morris-Pratt Algorithm当弄清楚了什么是部分匹配表之后，接下来就是怎么使用它，在匹配失败的时候进行适当的跳跃。使用参考这部分内容从头到尾彻底理解KMP\n最后给出一个实现\n跳表算法 &amp; 数据结构$$程序 &#x3D; 算法 + 数据结构$$TAOCP中，Donald并没有直接给出“算法”的定义，而是在用了几页的篇幅追溯了一番“Algorithms”这个词的起源以尝试让读者理解它的含义之后，用了欧几里得求解两个正整数最大公因子的例子做阐述：其中最重要的是→，赋值&#x2F;代替。他的学生、红皮算法书的作者-Robert Sedgewick沿用这个例子并尝试给出了一个定义：The term algorithm is used in computer science to describe a finite, deterministic, and effective problem-solving method suitable for implementation as a computer program.同时指出了二者之间的关系：大多数算法需要适当地组织数据，为了组织数据就产生了数据结构。一脉相承的观点是，数据结构是算法的副产品&#x2F;结果（data structures exist as the byproducts or end products of algorithms）。\n\n  数据结构的作用\n\n数据结构的作用前面说算法需要组织数据，所谓组织，其实就是操作（增、删、改、查）。有关数据结构和算法的课程对中所涉及到的数据结构：数组、链表；以及以前面两者为基础的高级数据结构：堆、树、图；延展开到特定领域&#x2F;方向上优化的数据结构：各种队列，红黑树，B、B+树，拓扑图等等。所有的数据结构的目的都是在特定场景下，优化数据的操作效率。可以用算法书给的demo跑一遍十几条排序算法的效率，便能直观感受到，即使在现在这样高性能的计算机面前，$n^2$ → $n\\log(n)$所带来巨大效率的提升；而在特定场景下，采用位图、$O(n)$复杂度的排序算法所能带来的更可观的空间、时间上的节省。绝大多数场景下，我们期待的数据结构是：在保持“有序”的前提下，满足高效的增、删、查操作。其中“有序”是一个相对的概念，堆、单端&#x2F;双端队列、查找树、拓扑图等，都满足以我们所期待的方式的有序性、或者我们所关心的那部分数据的有序性。\n\n\n哈希、红黑树、跳表这里关注K-V型数据结构。\n合适的数据结构关注以下速查表数据。其中，常用的key-value数据结构有三种：Hash表：插入、查找最快，为$O(1)$；如使用链表实现则可实现无锁；数据有序化需要显式的排序操作。红黑树：插入、查找为$\\log(n)$，但常数项较小；无锁实现的复杂性很高，一般需要加锁；数据天然有序。SkipList：插入、查找为$\\log(n)$，但常数项比红黑树要大；底层结构为链表，可无锁实现；数据天然有序。\n\n首先，如果能确定某些数据是静态的，以ACA为例，我们的文案数据目前就可以看成是静态的：可能有描述上的调整，但频次很低，并且数据量不大。这部分数据如果采用直接加载到内存或是中间缓存的话，结构化为HashMap是不错的选择；\n如前所述，大部分数据操作场景是需要增删改操作的，而非仅仅只有读操作。这里不再讨论堆、队列等使用场景，专注通常情况下的数据的存取操作，此时需要兼顾读取、和操作后恢复有序的效率，此时Hash表不再是好的选择：迭代、修改操作的时间复杂度比较高，\b而红黑树则能很好地满足功能需求；\n\n为什么还要有跳表作为平衡树的一种替代实现，跳表主要拥有以下优势：\n\n更简单的实现红黑树增删改元素需要进行旋转、变色，实现起来比较复杂，需要考虑的细节也比较多，到了并发场景下更难以写出可用且高效的红黑树实现；而跳表实现原理相当简单，就是升级版的链表，把链表的某一些元素随机抽出来再组成一个链表，作为一级索引，在该索引集中再次进行抽取，再做一级索引，依次实现多级链表索引，就组成了一个跳表。\n\n为了解决在高并发下，红黑树的锁实现导致的可能的死锁和并发度降低问题。首先这句话意味着，在单线程、低线程数场景下，红黑树可能是更好的选择：以jdk11为例，ConcurrentHashMap存取速度是ConcurrentSkipListMap的4倍左右，而随着并发的线程数增多，后者的性能优势会逐渐体现出来，它的存取时间复杂度几乎和线程数无关，且无锁开销。\n\n\n特点上述可见跳表也是一种典型的“空间换时间”的数据结构。其底层采用二维链表，而非通常采用的数组实现。基本特点：\n\n由很多层结构组成；\n每一层都是一个有序的链表；\n最底层(Level 1)的链表包含所有元素；\n如果一个元素出现在 Level i 的链表中，则它在 Level i 之下的链表也都会出现；\n每个节点包含两个指针，一个指向同一链表中的下一个元素，一个指向下面一层的元素。\n\n跳表实现构造考虑一个链表：从该有序表中搜索元素&lt; 23, 43, 59&gt;，需要比较的次数分别为&lt; 2, 4, 6 &gt;，总共比较的次数为 2 + 4 + 6 &#x3D; 12 次。有没有优化的算法？链表是有序的，但不能使用二分查找。类似二叉搜索树，我们把一些节点提取出来，作为索引。得到如下结构：这里把&lt; 14, 34, 50, 72 &gt;提取出来作为一级索引，这样搜索的时候就可以减少比较次数了。还可以再从一级索引提取一些元素出来，作为二级索引，变成如下结构：\n\n  节点类\n\nstatic final class Node&lt;K,V> &#123;\n    final K key; // currently, never detached\n    V val;\n    Node&lt;K,V> next;\n    Node(K key, V value, Node&lt;K,V> next) &#123;\n        this.key = key;\n        this.val = value;\n        this.next = next;\n    &#125;\n&#125;\n\nredis 使用C实现，详见：https://github.com/antirez/redis/blob/unstable/src/server.h\n\n\n搜索元素查找元素 117：\n\n比较21，比 21 大，往后面找\n比较37，比 37大，比链表最大值小，从 37 的下面一层开始找\n比较71，比 71 大，比链表最大值小，从 71 的下面一层开始找\n比较85，比 85 大，从后面找\n比较117，等于 117， 找到了节点。\n\n\n  搜索\n\nprivate Node&lt;K,V> findNode(Object key) &#123;\n    if (key == null)\n        throw new NullPointerException(); // don't postpone errors\n    Comparator&lt;? super K> cmp = comparator;\n    Node&lt;K,V> b;\n    outer: while ((b = findPredecessor(key, cmp)) != null) &#123;\n        for (;;) &#123;\n            Node&lt;K,V> n; K k; V v; int c;\n            if ((n = b.next) == null)\n                break outer;               // empty\n            else if ((k = n.key) == null)\n                break;                     // b is deleted\n            else if ((v = n.val) == null)\n                unlinkNode(b, n);          // n is deleted\n            else if ((c = cpr(cmp, key, k)) > 0)\n                b = n;\n            else if (c == 0)\n                return n;\n            else\n                break outer;\n        &#125;\n    &#125;\n    return null;\n&#125;\n\n\n新增元素先确定该元素要占据的层数 K（丢硬币，随机），然后在 Level 1 … Level K 各个层的链表都插入元素：插入 119， K &#x3D; 2其中，然随机变量 K 满足参数为 $p &#x3D; 1&#x2F;2$ 的几何分布，期望值 $E[K] &#x3D; 1&#x2F;p &#x3D; 2$。即各个元素的层数，期望值是 2 层。\n\n  插入\n\nprivate V doPut(K key, V value, boolean onlyIfAbsent) &#123;\n    if (key == null)\n        throw new NullPointerException();\n    Comparator&lt;? super K> cmp = comparator;\n    for (;;) &#123;\n        Index&lt;K,V> h; Node&lt;K,V> b;\n        VarHandle.acquireFence();\n        int levels = 0;                    // number of levels descended\n        if ((h = head) == null) &#123;          // try to initialize\n            Node&lt;K,V> base = new Node&lt;K,V>(null, null, null);\n            h = new Index&lt;K,V>(base, null, null);\n            b = (HEAD.compareAndSet(this, null, h)) ? base : null;\n        &#125;\n        else &#123;\n            for (Index&lt;K,V> q = h, r, d;;) &#123; // count while descending\n                while ((r = q.right) != null) &#123;\n                    Node&lt;K,V> p; K k;\n                    if ((p = r.node) == null || (k = p.key) == null ||\n                        p.val == null)\n                        RIGHT.compareAndSet(q, r, r.right);\n                    else if (cpr(cmp, key, k) > 0)\n                        q = r;\n                    else\n                        break;\n                &#125;\n                if ((d = q.down) != null) &#123;\n                    ++levels;\n                    q = d;\n                &#125;\n                else &#123;\n                    b = q.node;\n                    break;\n                &#125;\n            &#125;\n        &#125;\n        if (b != null) &#123;\n            Node&lt;K,V> z = null;              // new node, if inserted\n            for (;;) &#123;                       // find insertion point\n                Node&lt;K,V> n, p; K k; V v; int c;\n                if ((n = b.next) == null) &#123;\n                    if (b.key == null)       // if empty, type check key now\n                        cpr(cmp, key, key);\n                    c = -1;\n                &#125;\n                else if ((k = n.key) == null)\n                    break;                   // can't append; restart\n                else if ((v = n.val) == null) &#123;\n                    unlinkNode(b, n);\n                    c = 1;\n                &#125;\n                else if ((c = cpr(cmp, key, k)) > 0)\n                    b = n;\n                else if (c == 0 &amp;&amp;\n                         (onlyIfAbsent || VAL.compareAndSet(n, v, value)))\n                    return v;\n\n                if (c &lt; 0 &amp;&amp;\n                    NEXT.compareAndSet(b, n,\n                                       p = new Node&lt;K,V>(key, value, n))) &#123;\n                    z = p;\n                    break;\n                &#125;\n            &#125;\n\n            if (z != null) &#123;\n                int lr = ThreadLocalRandom.nextSecondarySeed();\n                if ((lr &amp; 0x3) == 0) &#123;       // add indices with 1/4 prob\n                    int hr = ThreadLocalRandom.nextSecondarySeed();\n                    long rnd = ((long)hr &lt;&lt; 32) | ((long)lr &amp; 0xffffffffL);\n                    int skips = levels;      // levels to descend before add\n                    Index&lt;K,V> x = null;\n                    for (;;) &#123;               // create at most 62 indices\n                        x = new Index&lt;K,V>(z, x, null);\n                        if (rnd >= 0L || --skips &lt; 0)\n                            break;\n                        else\n                            rnd &lt;&lt;= 1;\n                    &#125;\n                    if (addIndices(h, skips, x, cmp) &amp;&amp; skips &lt; 0 &amp;&amp;\n                        head == h) &#123;         // try to add new level\n                        Index&lt;K,V> hx = new Index&lt;K,V>(z, x, null);\n                        Index&lt;K,V> nh = new Index&lt;K,V>(h.node, h, hx);\n                        HEAD.compareAndSet(this, h, nh);\n                    &#125;\n                    if (z.val == null)       // deleted while adding indices\n                        findPredecessor(key, cmp); // clean\n                &#125;\n                addCount(1L);\n                return null;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n\n\n删除元素采用标准的链表删除即可。删除 71\n\n  删除\n\nfinal V doRemove(Object key, Object value) &#123;\n    if (key == null)\n        throw new NullPointerException();\n    Comparator&lt;? super K> cmp = comparator;\n    V result = null;\n    Node&lt;K,V> b;\n    outer: while ((b = findPredecessor(key, cmp)) != null &amp;&amp;\n                  result == null) &#123;\n        for (;;) &#123;\n            Node&lt;K,V> n; K k; V v; int c;\n            if ((n = b.next) == null)\n                break outer;\n            else if ((k = n.key) == null)\n                break;\n            else if ((v = n.val) == null)\n                unlinkNode(b, n);\n            else if ((c = cpr(cmp, key, k)) > 0)\n                b = n;\n            else if (c &lt; 0)\n                break outer;\n            else if (value != null &amp;&amp; !value.equals(v))\n                break outer;\n            else if (VAL.compareAndSet(n, v, null)) &#123;\n                result = v;\n                unlinkNode(b, n);\n                break; // loop to clean up\n            &#125;\n        &#125;\n    &#125;\n    if (result != null) &#123;\n        tryReduceLevel();\n        addCount(-1L);\n    &#125;\n    return result;\n&#125;\n\n\n适用场景\njdk从1.6开始引入了两个跳表相关的实现类：ConcurrentSkipListMap、ConcurrentSkipListSet（基于ConcurrentSkipListMap），在jdk中主要是用于高并发场景下代替红黑树的实现，不过从jdk8开始，线程安全的Hash表：ConcurrentHashMap采用了CAS、取消分段锁改用大数组、哈希碰撞超过阈值时树化（红黑树）等手段进一步提升了线程安全Hash表相关实现，性能上也有了很大提升。\n\nredis：redis的有序集合zset是采用跳表实现的。分析一下zset所支持的操作就不难理解为啥采用跳表而非红黑树了：\n\n\n\n插入元素\n删除元素\n查找元素\n有序输出所有元素\n查找区间内所有元素除了易于实现这个因素外。zset所支持的操作中，前4项红黑树都可以完成，且时间复杂度与跳表一致。但是，最后一项，红黑树的效率就没有跳表高了。在跳表中，要查找区间的元素，我们只要定位到两个区间端点在最低层级的位置，然后按顺序遍历元素就可以了，非常高效。而红黑树只能定位到端点后，再从首位置开始每次都要查找后继节点，相对来说是比较耗时的。\n\n\nLevelDB：Google 开源的 key&#x2F;value 存储引擎 LevelDB 以及 Facebook 基于 LevelDB 优化的 RocksDB 都是 LSM Tree 结构的数据库，内部的 MemTable 使用跳表实现。HBase MemStore 内部存储数据就使用的跳表。为什么呢？HBase 属于 LSM Tree 结构的数据库，LSM Tree 结构的数据库有个特点，实时写入的数据先写入到内存，内存达到阈值往磁盘 flush 的时候，会生成类似于 StoreFile 的有序文件，而跳表恰好就是天然有序的，所以在 flush 的时候效率很高，而且跳表查找、插入、删除性能都很高，这应该是 HBase MemStore 内部存储数据使用跳表的原因之一。HBase 使用的是 java.util.concurrent 下的 ConcurrentSkipListMap()。\n\nES：Lucene核心数据结构采用了跳表实现倒排表。使用FST保存词典，FST可以实现快速的Seek，这种结构在当查询可以表达成自动机时(PrefixQuery、FuzzyQuery、RegexpQuery等)效率很高。(可以理解成自动机取交集)此种场景主要用在对Query进行rewrite的时候。FST可以表达出Term倒排表所在的文件偏移。倒排表使用SkipList结构。从上面的讨论可知，求倒排表的交集、并集、差集需要各种SeekTo(docId)，SkipList能对Seek进行加速。\n\n\nref：https://stackoverflow.com/questions/256511/skip-list-vs-binary-search-treehttps://en.wikipedia.org/wiki/Skip_listhttps://blog.csdn.net/sunxianghuang/article/details/52221913https://www.iteye.com/blog/imtinx-1291165《algorithms》《the art of computer programming》\n","categories":["algs"],"tags":["algs"]},{"title":"CS","url":"/cs/cs/","content":"","categories":["cs"],"tags":["cs"]},{"title":"Spring","url":"/cs/spring/","content":"核心模块包括Bean、Context、Core。\nbeanspring的核心思想常常被称作BOP(Bean Oriented Programming)，面向Bean编程。Bean模块解决以下问题：\n\nbean的定义；\nbean的创建；\nbean的解析；\n\n作为业务开发通常只需要关心bean的创建，其他两个过程由spring内部完成。Bean的整体架构是典型的工厂模式，最上层的接口是BeanFactory。ListableBeanFactory、HierarchicalBeanFactory和AutowireCapableBean是其子类，目的是为了区分Spring内部对象处理和转化的数据限制：ListableBeanFactory: 表示这些Bean是可列表的，定义bean的集合；HierarchicalBeanFactory: 表示这些Bean有继承关系，定义bean的关系；AutowireCapableBeanFactory: 定义Bean的自动装配规则，定义bean的行为；\nbean定义，主要由BeanDefinition描述，成功解析后都会被转化为BeanDefinition对象，之后所有的操作都会在BeanDefinition对象之上进行。层次关系如下：\ncontextBean包装的是一个个Object，Object中存储着业务所需的数据。那么，如何给这些数据以及它们之间的关系提供生存、运行环境——即保存对象的状态，就是Context要解决的问题。Context就是bean关系的集合，即IoC容器。ApplicationContext是Context最上层的接口，层次关系如下：ApplicationContext能够标识一个应用环境的基本信息，继承了5个接口，用于拓展Context的功能，其中BeanFactory用于创建Bean，同时继承了ResourceLoader接口，用于访问任何外部资源。子类主要包括：\n\nConfigurableApplicationContext: 可动态配置和修改信息的Context，其下AbstractRefreshableApplicationContext最为常用。\nWebApplicationContext: 为Web应用准备的Context，可以直接访问ServletContext。\n\n作为Ioc容器，Context是Spring其他大部分功能的基础，ApplicationContext必须完成的功能包括：\n\n标识一个应用环境；\n利用BeanFactory创建Bean对象；\n保存对象关系表；\n捕获各种事件。\n\ncoreSpring发现、建立和维护Bean之间关系的一系列工具，实际上就是各种util。\nResource：core最重要的组成部分，主要定义了资源的访问方式，所有资源都抽象到了Resource接口中，主要作用：\n\n资源包装。Resource向上继承了InputStreamSource接口，所有的资源都通过InputStream来获取，从而屏蔽了资源提供者；\n资源加载。Resource下的ResourceLoader接口，所有资源加载者统一实现该接口就能加载所有的资源，例如之前的ApplicationContext。\n\n与Context建立关系：ApplicationContext通过ResourcePatternResolver接口与ResourceLoader进行交互，来进行资源的加载、解析和描述。ResourcePatternResolver将资源封装整合，便于其他模块使用。\nIoC 容器结构Spring IoC容器用于创建并管理Spring Bean对象以及Bean属性注入。通过ResourceLoader&#x2F;Resolver读取Bean的配置文件并转换成统一资源对象（Resource），然后通过BeanDefinitionReader转换成pring内部对Bean的描述对象（BeanDefinition），然后将其注册（BeanRegister）到容器中（BeanFactory），供以后转换成Bean对象使用。spring项目源代码很好地把它们划分到了不同的模块中，即上面的三个模块。而从资源读取、描述对象转换、注册使用这些阶段，从源码中又可以抽出以下6个组件：\n\n资源组件：Resource，对资源文件的描述，不同资源文件如xml、properties文件等，格式不同，最终都将被ResourceLoader加载获得相应的Resource对象；\n资源加载组件：ResourceLoader：加载xml、properties等各类格式文件，解析文件，并生成Resource对象；\nBean容器组件：BeanFactory体系，IoC容器的核心；\nBean注册组件：SingletonBeanRegister&#x2F;AliasRegister，将BeanDefinition对象注册到BeanFactory（BeanDefinition Map）中去；\nBean描述组件：BeanDefinition体系，Spring内部对Bean描述的基本数据结构；\nBean构造组件：BeanDefinitionReader体系，读取Resource并将其数据转换成一个个BeanDefinition对象。\n\nResource如各类型的文件，二进制流数据都是资源，是Spring内部对资源的统一描述，整个体系类图网如下：\nResourceLoaderResourceLoader&#x2F;Resolver体系，负责资源的加载，这里的资源指的是xml、properties等文件资源，返回一个对应类型的Resource对象。\nBeanDefinition对bean对象描述的基本数据结构。\nBeanDefinitionReaderBean构造组件，BeanDefinitionReader体系，将Resource对象，转换成BeanDefinition对象，就是将内部资源数据转换成Spring Bean描述数据。就是将统一资源数据对象读取转换成相应内部对象。\nSingletonBeanRegister&#x2F;AliasRegisterBean注册组件，将BeanDefinition对象注册到BeanFactory中。\nBeanFactoryBean容器组件，整个IoC容器核心，所谓Bean容器，就是这里装着Bean对象以及所需要的各种数据。其中BeanFactory是纯粹的Bean容器，用来存储描述Bean，无关其他环境，而像ApplicationContext，也是Bean容器，但它和应用环境息息相关，所以被称为应用上下文（环境）更恰当，从图中也能看出来，ApplicationContext不仅有着BeanFactory“血统”，同时也继承了EnvironmentCapable、MessageSource、ApplicationEventPublisher，即扩展了其许多额外功能，而其实现类则是和具体应用相关。\nSpringBoot启动过程请求处理过程以典型的web应用为例，一个请求从接收到返回的处理过程相当复杂，如下图所示，主要经过DispatcherServlet、HandlerMapping、Controller、ViewResolver、Model等：\n\n客户端请求提交到DispatcherServlet；\n由DisPatcherServlet控制器寻找一个或多个HandlerMapping，找到处理请求的Controller；\nDispatcherServlet将请求提交到Controller；\nController调用业务逻辑处理后返回ModelAndView；\nDispatcherServlet寻找一个或多个ViewResolver视图解析器，找到ModelAndView指定的视图；\n视图负责将结果显示到客户端。\n\nSpring MVC所有的请求都经过DispatcherServlet来统一分发，在 DispatcherServlet将请求分发给Controller之前需要借助Spring MVC提供的 HandlerMapping定位到具体的Controller。HandlerMapping接口负责完成客户请求到Controller映射。Controller接口将处理用户请求，这和Java Servlet扮演的角色是一致的。一旦Controller处理完用户请求，将返回ModelAndView对象给DispatcherServlet前端控制器，ModelAndView中包含了模型（Model）和视图（View）。从宏观角度考虑，DispatcherServlet是整个Web应用的控制器；从微观考虑，Controller是单个Http请求处理过程中的控制器，而ModelAndView是Http请求过程中返回的模型（Model）和视图（View）。ViewResolver接口（视图解析器）在Web应用中负责查找View对象，从而将相应结果渲染给客户。\n过滤器和拦截器这张图解释了典型spring应用常用组件及触发顺序：过滤器在请求进入容器之后、进入servlet之前就被触发，此时还没有进入spring的管辖范围。拦截器是spring提供并管理的，所以它能获取IoC容器信息，比如拿到容器里的bean。\n这张图解释了常用组件具体方法的执行顺序：\nfilterFilter是JavaEE中Servlet规范的一个组件，它可以在http请求到达Servlet之前，被N个Filter处理：过滤器的实现是基于函数回调。\n应用场景\n过滤敏感词汇（防止sql注入）\n设置字符编码\nURL级别的权限访问控制\n压缩响应信息\n\ninterceptor用于拦截Controller方法的执行，可以在方法执行、后添加自定义逻辑，类似于AOP编程思想。其实现是基于反射机制——动态代理。\n应用场景拦截器本质上是面向切面编程（AOP），符合横切关注点的功能都可以放在拦截器中来实现，主要的应用场景包括：\n\n登录验证，判断用户是否登录。\n权限验证，判断用户是否有权限访问资源，如校验token\n日志记录，记录请求操作日志（用户ip，访问时间等），以便统计请求访问量。\n处理cookie、本地化、国际化、主题等。\n性能监控，监控请求处理时长等。\n通用行为：读取cookie得到用户信息并将用户对象放入请求，从而方便后续流程使用，还有如提取Locale、Theme信息等，只要是多个处理器都需要的即可使用拦截器实现）\n\n后处理器BeanFactoryPostProcessor容器级别的后处理器。\nBeanPostProcessor监听器三级缓存https://cloud.tencent.com/developer/article/1497692\nbean生命周期IoC容器初始化过程：Spring容器中Bean的生命周期由多个特定的阶段组成，每个阶段都允许外界对Bean加以控制。在Spring中可以从两个层面定义Bean的生命周期：Bean的作用范围；实例化Bean时所经历的一系列阶段。\n文字描述\nBean容器在配置文件中找到Spring Bean的定义。\n\nBean容器使用Java Reflection API创建Bean的实例。\n\n如果声明了任何属性，声明的属性会被设置。如果属性本身是Bean，则将对其进行解析和设置。以上步骤见“spring IoC 源码解析”。\n\n如果Bean类实现BeanNameAware接口，则将通过传递Bean的名称来调用setBeanName()方法。如果Bean类实现BeanClassLoaderAware接口，则将通过传递加载此Bean的ClassLoader对象的实例来调用setBeanClassLoader()方法。如果Bean类实现BeanFactoryAware接口，则将通过传递BeanFactory对象的实例来调用setBeanFactory()方法。如果有任何与BeanFactory关联的BeanPostProcessors对象已加载Bean，则将在设置Bean属性之前调用postProcessBeforeInitialization()方法。如果Bean类实现了InitializingBean接口，则在设置了配置文件中定义的所有Bean属性后，将调用afterPropertiesSet()方法。如果配置文件中的Bean定义包含init-method属性，则该属性的值将解析为Bean类中的方法名称，并将调用该方法。如果为Bean Factory对象附加了任何Bean 后置处理器，则将调用postProcessAfterInitialization()方法。如果Bean类实现DisposableBean接口，则当Application不再需要Bean引用时，将调用destroy()方法。如果配置文件中的Bean定义包含destroy-method属性，那么将调用Bean类中的相应方法定义。\n\n\nBeanFactory中Bean的生命周期具体过程：\n\n当调用者通过getBean(beanName)向容器请求某一个Bean时，如果容器注册了org.springframework.beans.factory.config.InstantiationAwareBeanPostProcessor接口，则在实例化Bean之前，将调用接口的postProcessBeforeInstantiation()；\n根据配置情况调用Bean构造方法或工厂方法实例化Bean；\n如果容器注册了InstantiationAwareBeanPostProcessor接口，那么在实例化Bean之后，调用该接口的postProcessAfterInstantiation()，可在这里对已经实例化的对象进行一些“梳妆打扮”；\n如果Bean配置了属性信息，那么容器在这一步着手将配置值设置到Bean对应的属性中，不过在设置每个属性之前将先调用InstantiationAwareBeanPostProcessor接口的postProcessPropertyValues()；\n调用Bean的属性设置方法设置属性值；\n如果Bean实现了org.springframework.beans.factory.BeanNameAware接口，则将调用setBeanName()，将配置文件中该Bean对应的名称设置到Bean中；\n如果Bean实现了org.springframework.beans.factory.BeanFactoryAware接口，则将调用setBeanFactory()，将BeanFactory容器实例设置到Bean中；\n如果BeanFactory装配了org.springframework.beans.factory.config.BeanPostProcessor后处理器，则将调用BeanPostProcessor的Object postProcessBeforeInitialization(Object bean, String beanName)对Bean进行加工操作。其中，入参bean是当前正在处理的bean，beanName时当前bean的配置名，返回的对象为加工处理后的Bean。可以使用该方法对Bean进行处理，甚至改变Bean的行为。BeanPostProcessor在Spring框架中占有重要地位，为容器提供对Bean进行后续加工处理的切入点，Spring容器所提供的各种“神奇功能”，如AOP，动态代理等，都是通过它来实施的；\n如果Bean实现了InitializingBean接口，则将调用接口的afterPropertiesSet()；\n如果在&lt;bean&gt;中通过init-method属性定义了初始化方法，则将执行这个方法；\nBeanPostProcessor后处理器定义了两个方法：其一是postProcessBeforeInitialization()，在(8)步调用；其二是Object postProcessAfterInitialization(Object bean, String beanName)，这个方法在此时调用，容器再次获得对Bean进行加工处理的机会；\n如果在&lt;bean&gt;中指定Bean的作用范围是scope&#x3D;”prototype”，则将Bean返回给调用者，调用者负责Bean后续生命周期的管理，Spring不再管理这个Bean的生命周期。如果将作用范围设置为scope&#x3D;”singleton”，则将Bean放入Spring IoC容器的缓存池中，并将Bean引用返回给调用者，Spring继续对这些Bean进行后续的生命管理；\n对于scope&#x3D;”singleton”的Bean（默认情况），当容器关闭时，将触发Spring对Bean后续生命周期的管理工作。如果Bean实现了DisposableBean接口，则将调用接口的destory()，可以在此编写释放资源、记录日志等操作；\n对于scope&#x3D;”singleton”的Bean，如果通过&lt;bean&gt;的destory-method属性指定了Bean的销毁方法，那么Spring将执行Bean的这个方法，完成Bean资源的释放等操作。\n\nBean的完整生命周期从Spring容器着手实例化Bean开始，直到最终销毁Bean。其中经过了许多关键点，每个关键点都涉及特定的方法调用，可以将这些方法大致分为4类：\n\nBean自身的方法：如调用Bean构造方法实例化Bean、调用setter设置Bean的属性值以及通过&lt;bean&gt;的init-method和destory-method所指定的方法；\nBean级生命周期接口方法：如BeanNameAware、BeanFactoryAware、InitializingBean和DisposableBean，这些接口方法由Bean类直接实现；\n容器级生命周期接口方法：图示中⭐标识的步骤是由InstantiationAwareBeanPostProcessor和BeanPostProcessor这两个接口实现的，一般称它们的实现类为“后处理器”。后处理器接口一般不由Bean本身实现，它们独立于Bean，实现类以容器附加装置的形式注册到Spring容器中，并通过接口反射为Spring容器扫描识别。当Spring创建任何Bean的时候，这些后处理器都会发生作用，所以这些后处理器的影响是全局性的。也可以自行编写后处理器，让其仅对感兴趣的Bean进行加工处理；\n工厂后处理器接口方法：包括AspectJWeavingEnabler、CustomAutowireConfigurer、ConfigurationClassPostProcessor等方法。工厂后处理器也是容器级的，在应用上下文装配配置文件后立即调用。\n\nApplicationContext中Bean的生命周期Bean在应用上下文中的生命周期和在BeanFactory中的生命周期类似，不同的是，如果Bean实现了org.springframework.context.ApplicationContextAware接口，则会增加一个调用接口方法setApplicationContext()的步骤。  \n作用域Spring定义了多种作用域：\n\n单例（singleton），在整个应用中，只创建bean的一个实例。\n原型（prototype），每次注入或者通过Spring应用上下文获取的时候，都会创建一个新的bean实例。\n会话（session），在web应用中，为每个会话创建一个bean实例。\n请求（request），在web应用中，为每个请求创建一个bean实例。\n\n默认情况下，Spring的所有bean都是单例（singleton）的。使用@Scope注解来选择其它作用域，它可与@Component、@Bean一起使用。两种方式（推荐1）：\n1. \n@Component\n@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\npublic class AClass&#123;...&#125;\n\n@Component\n@Scope(\"prototype\")\npublic class AClass&#123;...&#125;\n\n@Override\npublic Object getBean(String name) throws BeansException &#123;\n\treturn doGetBean(name, null, null, false);\n&#125;\n\t\n在xxxConfig类中使用@Bean注解声明bean时和上述方式相同。会话和请求作用域，使用场景是web应用，从命名理解：会话作用域是指该bean的生命周期和一个会话的起始保持一致，比如典型的电商场景，用户登录，搜索商品，加入购物车，购买，付费，结束，这个流程就是一个完整的会话；请求作用域是指用户从发起一个请求开始，到服务器相应该请求的过程。这两种作用域的bean就是在该场景下，随着会话&#x2F;请求的生命周期实例化到销毁，典型的，如电商场景中的”购物车”。\n\nprotected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) &#123;\n\t// Make sure bean class is actually resolved at this point.\n\t// 解析出 Class\n\tClass&lt;?> beanClass = resolveBeanClass(mbd, beanName);\n\n\tif (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123;\n\t\tthrow new BeanCreationException(mbd.getResourceDescription(), beanName,\n\t\t\t\t\"Bean class isn't public, and non-public access not allowed: \" + beanClass.getName());\n\t&#125;\n\n\t// 如果工厂方法不为空，则是用工厂方法初始化\n\tif (mbd.getFactoryMethodName() != null)  &#123;\n\t\t// 相关知识点看另一篇文章关于FactoryBean的\n\t\treturn instantiateUsingFactoryMethod(beanName, mbd, args);\n\t&#125;\n\n\t// Shortcut when re-creating the same bean...\n\t// 如果不是第一次创建，比如第二次创建 prototype bean。\n\t// 这种情况下，我们可以从第一次创建知道，采用无参构造函数，还是构造函数依赖注入 来完成实例化\n\t// 所以注释说叫shortcut\n\tboolean resolved = false;\n\tboolean autowireNecessary = false;\n\tif (args == null) &#123;\n\t\tsynchronized (mbd.constructorArgumentLock) &#123;\n\t\t\tif (mbd.resolvedConstructorOrFactoryMethod != null) &#123;\n\t\t\t\t// 有已经解析过的构造方法\n\t\t\t\tresolved = true;\n\t\t\t\tautowireNecessary = mbd.constructorArgumentsResolved;\n\t\t\t&#125;\n\t\t&#125;\n\t&#125;\n\t// 如果已经解析过则使用解析好的构造方法不需要再次锁定\n\tif (resolved) &#123;\n\t\tif (autowireNecessary) &#123;\n\t\t\t// 构造方法自动注入\n\t\t\treturn autowireConstructor(beanName, mbd, null, null);\n\t\t&#125;\n\t\telse &#123;\n\t\t\t// 默认构造方法\n\t\t\treturn instantiateBean(beanName, mbd);\n\t\t&#125;\n\t&#125;\n\n\t// Need to determine the constructor...\n\t// 判断是否采用有参构造函数\n\t// 构造器自动装配\n\tConstructor&lt;?>[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName);\n\tif (ctors != null ||\n\t\t\tmbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR ||\n\t\t\tmbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args))  &#123;\n\t\treturn autowireConstructor(beanName, mbd, ctors, args);\n\t&#125;\n\n\t// No special handling: simply use no-arg constructor.\n\t// 使用无参构造器\n\treturn instantiateBean(beanName, mbd);\n&#125;\n\n\n\nBeanFactory vs FactoryBean首先，它们都是spring framework里比较顶层的接口：BeanFactory可以看做最简版的容器形式，也给具体的IOC容器实现提供规范，比如ApplicationContext；FactoryBean为IOC容器中的Bean创建提供更加灵活的方式，通过给Bean的实现加上一个简单的工厂模式和装饰器模式，使得对Bean的配置更加便捷灵活。从另一个角度讲，两者都是工厂，FactoryBean本身也是一个Factory，并且归BeanFactory管理。\nBeanFactory以Factory结尾的都是工厂类&#x2F;接口。它是IOC容器的核心接口，其职责包括：实例化、定位、配置应用程序中的对象，以及建立这些对象间的依赖。FactoryBean只是一个接口，并非IOC容器的具体实现，但spring给出了很多种实现：ApplicationContext、DefaultListableBeanFactory、AnnotationConfigApplicationContext等。其中AnnotationConfigApplicationContext是目前构建具体应用很常用的一个，它实现将以注解方式描述组成应用的对象和对象间的依赖关系。AnnotationConfigApplicationContext类将持有注解配置的所有元数据信息，并用这些元数据构建一个完全可配置的系统或应用。再如，ApplicationContext，派生自BeanFactory，仍然是一个接口，但它提供一种更面向框架的方式工作，并且对上下文进行分层和实现继承关系。同时它还派生自MessageSource、ApplicationEventPublisher、HierarchicalBeanFactory、ResourcePatternResolver等接口，从而扩充了以下能力：\n\nMessageSource, 提供国际化的消息访问;\n资源访问，如URL和文件;\n事件传播;\n载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层;\n\n其他各种BeanFactory的子接口、子类也都是扩展了某种功能的类工厂，用于特定场景。\n主要方法：\n使用场景\n从IOC容器中获取Bean(byName or byType)\n检索IOC容器中是否包含指定的Bean\n判断Bean是否为单例\n\nFactoryBean首先它是一个Bean，用于生产&#x2F;修饰其他Bean的工厂Bean，表现为一个工厂的职责，主要用到工厂模式、装饰器模式。通常情况下，spring容器担任工厂的角色，但也存在一些场景，比如某些Bean实例化过程复杂，需要配置大量信息，此时可以提供一个FactoryBean，用它来实现定制化的Bean实例化逻辑。FactoryBean接口对于Spring框架来说有着重要地位，自身就提供了70多个FactoryBean的实现。\n主要方法：\n使用场景FactoryBean在Spring中最为典型的一个应用就是用来创建AOP的代理对象。AOP实际上是Spring在运行时创建了一个代理对象，也就是说这个对象是在运行时创建的，而不是一开始就定义好的，这很符合工厂方法模式。更形象地说，AOP代理对象通过Java的反射机制，在运行时创建了一个代理对象，在代理对象的目标方法中根据业务要求织入了相应的方法。这个对象在Spring中就是——ProxyFactoryBean。\nIoCInversion of Control，控制反转，也称依赖注入（DI，Dependency Injection）。应用对象之间的解耦。通常所讨论的依赖注入是将一个bean的引用注入到另一个bean的属性或构造器参数中，即将一个对象与另一个对象关联起来。java的反射是实现依赖注入的底层技术。依赖注入是Spring容器的内核，AOP、声明式事务等功能也都基于此。Spring是一种容器框架，它帮助完成类的初始化和装配工作，让开发者从这些底层实现类的实例化、依赖关系装配等工作中解脱出来，专注于业务逻辑的开发工作。它通过配置文件或注解描述类和类之间的依赖关系，利用java的反射功能实例化Bean并建立Bean之间的依赖关系，自动完成类的初始化和依赖注入工作。此外，Spring还提供Bean实例缓存、生命周期管理、Bean实例代理、事件发布、资源装载等高级服务。BeanFactory(com.springframework.beans.factory.BeanFactory)是Spring框架最核心的接口，它提供了高级IoC的配置机制。BeanFactory使管理不同类型的java对象成为可能，应用上下文(com.springframework.context.ApplicationContext)建立在BeanFactory基础之上，提供了更多面向应用的功能，如i18n和框架事件体系等，更易于创建实际应用。一般称BeanFactory为IoC容器，称ApplicationContext为应用上下文，有时也称后者为Spring容器。用途上，BeanFactory是Spring框架的基础设施，面向Spring本身；ApplicationContext面向使用Spring框架的开发者，几乎所有的应用场合都可以直接使用ApplicationContext而非底层的BeanFactory。\nApplicationContext如果说BeanFactory是Spring的”心脏“，那么ApplicationContext就是完整的”身躯“。ApplicationContext由BeanFactory派生而来，提供了更多面向实际应用的功能。在BeanFactory中，很多功能需要以编程的方式实现，而在ApplicationContext中则可以通过配置的方式实现。ApplicationContext的主要实现类是ClassPathXmlApplicationContext和FileSystemXmlApplicationContext，前者默认从类路径加载配置文件，后者默认从文件系统中装载配置文件。可见，除继承HierarchicalBeanFactory和ListableBeanFactory接口外，ApplicationContext还通过多个其它接口扩展BeanFactory的功能：\n\nApplicationEventPublisher：让容器拥有发布应用上下文事件的功能，包括容器启动事件、关闭事件等。实现了ApplicationListener事件监听接口的Bean可以接收到容器事件，并对事件进行响应处理。在ApplicationContext抽象实现类AbstractApplicationContext中存在一个ApplicationEventMulticaster，它负责保存所有的监听器，以便在容器产生上下文事件时通知这些事件监听者；\nMessageSource：为应用提供i18n国际化消息访问的功能；\nResourcePatternResolver：所有ApplicationContext实现类都实现了类似于PathMatchingResourcePatternResolver的功能，可以通过带前缀的ant风格的资源文件路径装载Spring配置文件；\nLifeCycle：提供start()、和stop()，主要用于控制异步处理过程。在具体使用时，该接口同时被ApplicationContext实现以及具体Bean实现，ApplicationContext会将start&#x2F;stop的信息传递给容器中所有实现了该接口的Bean，以达到管理和控制JMX、任务调度等目的。\n\nConfigurableApplicationContext扩展于ApplicationContext，它新增了两个主要方法：refresh()、close()，使得ApplicationContext具有启动、刷新、关闭应用上下文的能力。在应用上下文关闭的情况下可以调用refresh()即可启动应用上下文，在已经启动的状态下调用refresh()则可清除缓存并重新装载配置信息，而调用close()则可关闭应用上下文。这些接口方法为容器的控制管理带来了便利，但作为开发者并不需要过多关心它们。  \n初始化和BeanFactory初始化类似，如果配置文件放在类路径下，优先考虑使用ClassPathXmlApplicationContext实现；如果放在文件系统路径下，则优先考虑FileSystemXmlApplicationContext实现。也可以指定一组配置文件，Spring会完成自动整合\u0010。获取ApplicationContext实例后，就可以像BeanFactory一样调用getBean(beaName)返回bean了。而ApplicationContext的初始化和BeanFactory有一个很大区别：后者在初始化容器时，并未初始化所有的Bean，直到第一次访问某个Bean时才实例化该目标Bean；而ApplicationContext则在初始化应用上下文时就实例化所有单实例的Bean。因此，ApplicationContext的初始化时间会比BeanFactory稍长。Spring支持基于类注解的配置方式，主要功能来自JavaConfig的子项目。一个标注了@Configuration注解的POJO即可提供Spring所需的Bean配置信息。而且Spring为基于注解类的配置专门提供了ApplicationContext实现类：AnnotationConfigApplicationContext，可以直接调用方法实例化Bean，启动容器并装配Bean。\nWebApplicationContextWebApplicationContext是专门为Web应用准备的，它允许从相对于Web根目录的路径中装载配置文件完成初始化工作。从WebApplicationContext中可以获得ServletContext的引用，整个Web应用上下文对象将作为属性放置到ServletContext中，以便Web应用环境可以访问Spring应用上下文。Spring专门为此提供了一个工具类WebApplicationContextUtils，通过该类的getWebApplicationContext(ServletContext sc)，可以从ServletContext中获取WebApplicationContext实例。在非Web应用的环境下，Bean只有singleton、prototype两种作用域。WebApplicationContext为Bean添加了三个新的作用域：request、session、global session。ConfigurableWebApplicationContext扩展了WebApplicationContext，它允许通过配置的方式实例化WebApplicationContext，同时定义了两个重要方法：\n\nsetServletContext(ServletContext servletContext)：为Spring设置Web应用上下文，以便二者整合；\nsetConfigLocations(String[] configLocations)：设置Spring配置文件地址，一般是相对于Web根目录的地址，如&#x2F;WEB-INF&#x2F;xxx-dao.xml、&#x2F;WEB-INF&#x2F;xxx-service.xml。用户也可以使用带资源类型前缀的地址，如classpath:com&#x2F;baidu&#x2F;beans.xml等。\n\n初始化WebApplicationContext的初始化方式和BeanFactory、ApplicationContext有所区别，因为需要ServletContext实例。即必须在拥有Web容器的前提下才能完成启动工作，可以在web.xml中配置自启动的Servlet或定义Web容器监听器(ServletContextListener)，二者均可完成启动Spring Web应用上下文的工作。Spring为二者均提供了支持：\n\norg.springframework.web.context.ContextLoaderServlet\norg.springframework.web.context.ContextLoaderListener\n\n二者内部都实现了启动WebApplicationContext实例的逻辑，只需根据Web容器的具体情况选择其一并在web.xml中完成配置即可。\nWebApplicationContext同样需要日志功能，可以将Log4J配置文件放置在类路径&#x2F;WEB-INF&#x2F;classes下以便启动Log4J引擎。或在web.xml文件指定自定义的配置文件位置。\nAOPAOP是一种编程思想。日志、安全、事务管理、缓存等，在软件系统中都是非常重要的功能，但它们与软件本身所关注的“功能”即业务逻辑，从概念上讲（应该）是分离的，然而它们散布嵌入在业务逻辑之中，需要在业务逻辑功能执行的过程中被动地触发。这些功能通常被称为横切关注点（cross-cutting concern）。把这些横切关注点与业务逻辑相分离就是面向切面编程（AOP，Aspect Oriented Programming），实现横切关注点与它们所影响的对象之间的解耦。重用通用功能的方案一般为继承或委托，而切面是另一种实现该目标的方案。在使用面向切面编程时，仍然在一个地方定义通用功能，但是可以通过声明的方式定义这个功能要以合何种方式在何处应用，而无需修改受影响的类。横切关注点可以被模块化为特殊的类，这些类被称为切面（aspect）。有两个好处：\n\n每个关注点都集中在一个地方，而不是分散在多处代码中；\n服务模块更加简洁，因为它们都只包含主要关注点（核心功能）的代码，而次要关注的代码被转移到切面中。\n\n相关术语增强 advice切面的工作被称为增强。增强定义了切面是什么以及何时使用（what and when）。除了描述切面要完成的工作，增强还解决了何时执行这个工作的问题：它应该应用在某个方法被调用之前、之后还是只在方法抛出异常时，等。Spring切面支持5种类型的增强：\n\n前置增强（Before）：在目标方法被调用之前调用增强功能；\n后置增强（After）：在目标方法完成之后调用增强，此时不会关心方法的输出是什么；\n环绕增强（Around）：在被增强方法调用前后都执行自定义的行为；\n返回增强（After-returning）：在目标方法成功执行之后调用增强；\n异常增强（After-throwing）：在目标方法抛出异常之后调用增强。\n\n连接点 join point应用增强的时机，被称为连接点。连接点是在应用执行过程中能够插入切面的一个点，这个点可以是调用方法时、抛出异常时、甚至修改一个字段时，切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。\n切点 pointcut增强定义了切面的“what”和“when”，切点定义了切面的“where”，切点的定义会匹配增强所要织入的一个或多个连接点。通常使用明确的类和方法名，或利用正则表达式定义所匹配的类和方法名来指定这些切点。\n切面 aspect切面是增强和切点的结合。增强和切点共同定义了切面的全部内容——它是什么，在何时何处完成其功能。 \n引入 introduction引入允许向现有的类添加新方法或属性，从而在无需修改现有类的情况下，让这些类具有新的行为和状态。是一种特殊的增强。\n织入 weaving把切面应用到目标对象并创建新的代理对象的过程。切面在指定的连接点被织入到目标对象中，在目标对象的生命周期里有多个点可以进行织入：\n\n编译期：切面在目标类编译时被织入。这种方式需要特殊的编译器。AspectJ的织入编译器就是以这种方式织入切面的。\n类加载期：切面在目标类加载到JVM时被织入。这种方式需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ 5的加载时织入（load-time weaving，LTW）就支持以这种方式织入切面。\n运行期：切面在应用运行的某个时刻被织入。一般，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。Spring AOP就是以这种方式织入切面的。\n\nSpring中的AOPAOP框架在连接点模型上有强弱之分，比如有些允许在字段修饰符级别应用增强，有些只支持与方法调用相关的连接点。此外，框架织入切面的方式和时机也有所不同。但无论如何，创建切点来定义切面所织入的连接点是AOP框架的基本功能。\nSpring提供4种类型的AOP支持：\n\n基于代理的经典Spring AOP（现在来看笨重且复杂）；\n纯POJO切面（需要xml配置）；\n@AspectJ注解驱动的切面（本质上依然是基于代理的AOP）；\n注入式AspectJ切面（基本的方法调用级别切面满足不了需求时）。\n\n前三种都是Spring AOP实现的变体。Spring AOP构建在动态代理基础之上，因此，Spring对AOP的支持局限于方法拦截。\n运行时增强对象通过在代理类中包裹切面，Spring在运行期把切面织入到Spring管理的bean中。代理类封装了目标类，拦截被增强方法的调用，再把调用转发给真正的目标bean。即代理类处理方法的调用，执行额外的切面逻辑，并调用目标方法。Spring运行时才创建代理对象。\n切点类型Spring提供6种类型的切点：\n\n静态方法切点：org.springframework.aop.support.StaticMethodMatcherPointcut是静态方法切点的抽象基类，默认情况下它可以匹配所有的类。包含两个主要的子类：NameMatchMethodPointcut &amp; AbstractRegexpMethodPointcut，前者提供简单的字符串匹配方法签名，后者使用正则表达式匹配方法签名；\n动态方法切点：org.springframework.aop.support.DynamicMethodMatcherPointcut是动态方法切点的抽象基类，默认情况下匹配所有的类；\n注解切点：org.springframework.aop.support.annotation.AnnotationMatchingPointcut的实现类表示注解切点，支持在Bean中直接通过java5.0注解标签定义的切点；\n表达式切点：org.springframework.aop.support.ExpressionPointcut接口主要是为了支持AspectJ切点表达式语法而定义的接口；\n流程切点：org.springframework.aop.support.ControlFlowPointcut实现类表示控制流程切点。ControlFlowPointcut是一类特殊的切点，它根据程序执行堆栈的信息查看目标方法\n\n","categories":["java"],"tags":["java"]},{"title":"中间件","url":"/cs/mw/","content":"redisremote dictionary server, redis, 是一个开源的内存中的数据结构存储系统，它可以用作：数据库、缓存和消息中间件。采用 单进程单线程 方式运行。Redis 支持多种类型的数据结构，如字符串（Strings），散列（Hash），列表（List），集合（Set），有序集合（Sorted Set或者是ZSet）与范围查询，Bitmaps，Hyperloglogs 和地理空间（Geospatial）索引半径查询。其中常见的数据结构类型有：String、List、Set、Hash、ZSet这5种。redis 教程\n查询速度Redis 采用基于内存的、单进程单线程模型的 KV 数据库，由 C 语言编写。官方提供的数据是可以达到 100000+ 的 QPS（每秒内查询次数）。\n高效查询的原因\n纯内存操作，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1)；\n数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的；\n采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；\n使用多路 I&#x2F;O 复用模型，非阻塞 IO；\n底层模型不同，Redis 直接构建了专用的 VM 机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。\n\n单线程的问题首先要明确，这里单线程是指，Redis 只使用一个线程来处理网络请求，而一个 Redis Server 处于运行状态时还是有多个线程的，比如用于持久化的子线程&#x2F;子进程。此外，进入 4.0 版本后，某些操作上开始支持多线程执行，不排除后续版本将整个服务都采用多线程的可能。显然，单线程无法发挥多核的优势，通常在单机开启多个 Redis 实例以充分利用多核环境资源。为什么说Redis是单线程的\n基于 Key-Value 的 NoSQL 内存数据库也称数据结构服务器。特点和优势：\n\n支持数据的持久化，可以将内存中的数据持久化到磁盘中，重启时再次加载使用；\n不仅支持简单的 key-value 类型数据，同时还提供 list、set、zset、hash 等数据结构的存储；\n支持数据备份，即 master-slave 模式的数据备份；\n性能极高：读的速度是110000次&#x2F;s,写的速度是81000次&#x2F;s；\n丰富的数据类型，提供了5种数据结构：String、List、Hash、Set、Ordered Set（ZSet）；\n原子操作：Redis 的所有操作都是原子性的，同时还支持对几个操作全并后的原子性执行；\n丰富的特性：支持 publish&#x2F;subscribe、通知 key 过期等特性。\n\n数据结构\nstring类似于其他编程语言中字符串的概念。redis 中的 string 类型是二进制安全的，i.e. 可以包含任何数据，比如一张 .jpg 格式的图片。一个键最大存储量为 512 MB。\nlist按照插入顺序有序存储多个字符串，相同元素可重复，双向操作（LPHSH、LPOP、RPUSH、RPOP）。每个 list 最多存储元素数量：2^32 - 1。\nset集合和列表都可以存储多个字符串，不同之处在于：列表可以存储多个相同的字符串，集合通过 散列表来保证存储的每个字符串都是不相同的。redis 的集合使用无序（unordered）方式存储元素，不支持像列表一样将元素从某一端 push&#x2F;pop 的操作，相应地，使用 SADD&#x2F;SREM 添加&#x2F;移除元素。由于是通过哈希表实现的，所以添加&#x2F;移除&#x2F;查找的时间复杂度为 O(1)。每个 set 最多存储元素数量：2^32 - 1。\nhash可以存储多个键值对之间的映射。官方推荐：尽可能使用hash存储数据。每个 hash 最多存储键值对数量：2^32 - 1。\nzset有序集合（zset）和散列一样，都用于存储键值对，不支持重复元素。不同之处在于：有序集合的键被称为成员（member），每个成员都是各不相同的；值被称为分值（score），必须为浮点数（分值可重复）。zset 既可以根据成员访问元素（和散列一样），又可以根据分值以及分值的排列顺序来访问元素的结构。每个 zset 最多存储键值对数量：2^32 - 1。\nuse case作为分布式锁分布式锁至少要满足三个属性要求：\n\n安全方面（Safety property）：互斥。在任一时刻，只有一个client可以获取锁；\n活性A（Liveness property）：无死锁。即便持有锁的client崩溃（crashed)或者网络被分裂（gets partitioned)，锁仍然可以被获取；\n活性B（Liveness property）：容错。只要多数Redis节点活着，client就可以获取和释放锁。\n\n参见 redlock\n作为LRU缓存redis提供多种key淘汰机制：  \n\nnoeviction: 不淘汰，当超过内存限制，抛出异常；\nallkeys-lru: 在所有键中，选取最近最少使用的数据抛弃；\nvolatile-lru: 在设置了过期时间的所有键中，选取最近最少使用的数据抛弃；\nallkeys-random: 在所有键中，随机抛弃；\nvolatile-random: 在设置了过期时间的所有键中，随机抛弃；\nvolatile-ttl: 在设置了过期时间的所有键中，抛弃存活时间最短的数据；4.0 新增：\nallkeys-lfu: 在所有键中，选取使用频率最少的数据抛弃；\nvolatile-lfu: 在设置了过期时间的所有键中，选取使用频率最少的数据抛弃；\n\nPersistenceredis提供不同的持久化选项：  \n\nRDB: 在指定的时间间隔内将内存中的数据集快照写入磁盘；\nAOF(append only file): 将server接收到的每个操作日志以追加的方式写入文件；\n\n多线程6.0版本开始支持多线程。\n","categories":["mw"],"tags":["mw"]},{"title":"about","url":"/about/index.html","content":"A computer programmer based in hangzhou, China.@alibaba.Ex@kwai.Ex@baidu ABC.\ngithub:https://github.com/lazy-snail\n","categories":[],"tags":[]}]